{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science] \n",
    "## 2-3 Regression - Student Version\n",
    "\n",
    "In this lab, we are going to cover **Regression** methods. Supervised machine learning can be divided into classification and regression. We will begin with regression as a review of your previous statistics courses. This lab will introduce the regression methods available in the scikit-learn extension to scipy, focusing on ordinary least squares linear regression, LASSO, and Ridge regression.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "\n",
    "1 - [Data Splitting Review](#section_1)\n",
    "\n",
    "2 - [Linear Regression](#section_2)\n",
    "\n",
    "3 - [Ridge Regression](#section_3)\n",
    "\n",
    "4 - [LASSO Regression](#section_4)\n",
    "\n",
    "5 - [Hyperparameter Tuning](#section_5)\n",
    "\n",
    "6 - [Choosing a Model](#section_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environment\n",
    "Remember to always activate your virtual environment first before you install packages or run a notebook! This helps to prevent conflicts between dependencies across different projects and ensures that you are using the correct versions of packages. You must have created anaconda virtual enviornment in the `Anaconda Installation` lab. If you have not or want to create a new virtual environment, follow the instruction in the `Anaconda Installation` lab. \n",
    "\n",
    "<br>\n",
    "\n",
    "If you have already created a virtual enviornment, you can run the following command to activate it: \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate <virtual_env_name>`\n",
    "\n",
    "<br>\n",
    "\n",
    "For example, if your virtual environment was named as CSS, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate CSS`\n",
    "\n",
    "<br>\n",
    "\n",
    "To deactivate your virtual environment after you are done working with the lab, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda deactivate`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries \n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# use magic function\n",
    "%matplotlib inline\n",
    "\n",
    "# set style\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data: Bike Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your time at Cal, you've probably passed by one of the many bike sharing station around campus. Bike sharing systems have become more and more popular as traffic and concerns about global warming rise. This lab's data describes one such bike sharing system in Washington D.C., from [UC Irvine's Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load data \n",
    "bike = pd.read_csv('../../data/day.csv')\n",
    "\n",
    "# drop problematic columns\n",
    "bike = bike.drop(['instant', 'dteday', 'weekday'], axis=1)\n",
    "\n",
    "# make season categorical\n",
    "bike['season'] = bike['season'].replace([1, 2, 3, 4],\n",
    "                                        ['winter', 'spring', 'summer', 'fall'])\n",
    "\n",
    "# make weathersit categorical\n",
    "bike['weathersit'] = bike['weathersit'].replace([1, 2, 3, 4],\n",
    "                                                ['favorable', 'misty', 'light', 'heavy'])\n",
    "\n",
    "\n",
    "# get dummies from categorical variables\n",
    "bike = pd.get_dummies(data=bike)\n",
    "\n",
    "# inspect\n",
    "bike.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to get familiar with the data set. In data science, you'll often hear rows referred to as **records** and columns as **features**. Before you continue, explore the dataset and make sure you can answer the following:\n",
    "\n",
    "**QUESTIONS:**\n",
    "\n",
    "- How many records are in this data set?\n",
    "- What does each record represent? (How is the data structured?)\n",
    "- What are the different features?\n",
    "- How is each feature represented? What values does it take, and what are the data types of each value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the data set here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWERS:**\n",
    "\n",
    "- How many records are in this data set? **.......**\n",
    "\n",
    "- What does each record represent? (How is the data structured?) **.......**\n",
    "- What are the different features? **.......**\n",
    "- How is each feature represented? What values does it take, and what are the data types of each value? **.......**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Test-Train-Validation Split  <a id='section_1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we want to see what factors affect **ridership**. But first, recall from last week that before we make predictions, we need to split our data first. \n",
    "\n",
    "So, let's prepare the bike dataset by creating a dataframe **X** with all of the features (exclude anything that is not a rider count), and a series, **y** with the *total number of riders*. \n",
    "\n",
    "*Remeber to drop one reference category of the dummies we made.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features used to predict riders\n",
    "X = bike.drop(... ,                           # list of variables to drop\n",
    "              axis= ...)                      # which axis do we want to specify here\n",
    "    \n",
    "# the number of riders\n",
    "y = bike.cnt                                 # new way to subset a column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set the random seed using `np.random.seed(...)`. This will affect the way numpy pseudo-randomly generates the numbers it uses to decide how to split the data into training and test sets. Any seed number is fine- the important thing is to document the number you used in case we need to recreate this pseudorandom split in the future.\n",
    "\n",
    "Then, call `train_test_split` on your X and y. Also set the parameters `train_size=` and `test_size=` to set aside 80% of the data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# split the data so that it returns 4 values: X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(...,              # specify training dataset\n",
    "                                                    ...,              # specify test dataset\n",
    "                                                    train_size=...,   # specify proportional split for training\n",
    "                                                    test_size=...)    # specify proportional split for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Validation Set\n",
    "\n",
    "Recall that our test data should only be used once: after our model has been selected, trained, and tweaked. Unfortunately, it's possible that in the process of tweaking our model, we could still overfit it to the training data and only find out when we return a poor test data score. What then?\n",
    "\n",
    "A **validation set** can help here. By trying your trained models on a validation set, you can (hopefully) weed out models that don't generalize well.\n",
    "\n",
    "Call `train_test_split` again, this time on your `X_train` and `y_train`. We want to set aside 25% of the data to go to our validation set, and keep the remaining 75% for our training set.\n",
    "\n",
    "Note: This means that out of the original data, 20% is for testing, 20% is for validation, and 60% is for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data so that it returns 4 values: X_train, X_validate, y_train, y_validate\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(..., \n",
    "                                                            ..., \n",
    "                                                            train_size=..., \n",
    "                                                            test_size=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing/Standardizing\n",
    "\n",
    "Recall that one of the important preprocessing steps is to normalize or standardize features that are numeric. The best time to do this is **AFTER** the training/validation/test split to avoid any information from the test data making its way into the training data. This is to avoid what is called  [data leakage](https://machinelearningmastery.com/data-preparation-without-data-leakage/), which occurs in this instance because if you normalize/standardize before the split, then information about the larger distribution for each feature (which included test observations) will by definition be contained in the standardize/normalized training data.\n",
    "\n",
    "To avoid this, we standardize/normalize **AFTER** the split. There are two common practices to do this: \n",
    "\n",
    "- **standardization:** (Z-score standardization), where you subtract the mean of from each value and divde by the standard deviation \n",
    "- **normalization:** (min-max normalization), where you subtract the minimum value from each value and divide by the range (max - min).  \n",
    "\n",
    "\n",
    "Not going to run these here since the original data was normalized. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Standardization\n",
    "#-----------\n",
    "\n",
    "# load library \n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "#\n",
    "## scale the training and validation datasets\n",
    "#X_train_s = scaler.fit_transform(X_train)        # fit and transform the training data\n",
    "#X_validate_s = scaler.fit_transform(X_validate)  # fit and transform the validation data\n",
    "#\n",
    "## scale the test dataset\n",
    "#X_test_s = scaler.fit_transform(X_test)\n",
    "#\n",
    "#\n",
    "## convert to datframes\n",
    "#X_train_s_df = pd.DataFrame(X_train_s, columns=X_train.columns)\n",
    "#X_validate_s_df = pd.DataFrame(X_validate_s, columns=X_validate.columns)\n",
    "#X_test_s_df = pd.DataFrame(X_test_s, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Normalization\n",
    "#-----------\n",
    "\n",
    "# load library \n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "#\n",
    "## scale the training and validation datasets\n",
    "#X_train_s = scaler.fit_transform(X_train)        # fit and transform the training data\n",
    "#X_validate_s = scaler.fit_transform(X_validate)  # fit and transform the validation data\n",
    "#\n",
    "## scale the test dataset\n",
    "#X_test_s = scaler.fit_transform(X_test)\n",
    "#\n",
    "#\n",
    "## convert to datframes\n",
    "#X_train_s_df = pd.DataFrame(X_train_s, columns=X_train.columns)\n",
    "#X_validate_s_df = pd.DataFrame(X_validate_s, columns=X_validate.columns)\n",
    "#X_test_s_df = pd.DataFrame(X_test_s, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression (Ordinary Least Squares) <a id='section_2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to start training models and making predictions. We'll start with a **linear regression** model.\n",
    "\n",
    "[Scikit-learn's linear regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score) is built around scipy's ordinary least squares, which you used in the last lab. The syntax for each scikit-learn model is very similar:\n",
    "1. Create a model by calling its constructor function. For example, `LinearRegression()` makes a linear regression model.\n",
    "2. Train the model on your training data by calling `.fit(X_train, y_train)` on the model\n",
    "\n",
    "Create a linear regression model in the cell below, and fit it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "lin_model = lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model fit, you can look at the best-fit slope for each feature using `.coef_`, and you can get the intercept of the regression line with `.intercept_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model coefficients and intercept\n",
    "print(lin_model.coef_)\n",
    "print(lin_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the coefficients. Fill in the code below to produce a bar plot for the coefficients.\n",
    "Reminder: Seaborn's barplot takes arguments for `x`, `y` and `data`. You can take `x` and `y` from the DataFrame created below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the coefficient and feature names for plotting\n",
    "lin_reg_data = pd.DataFrame([lin_model.coef_, X.columns]).T # make a dataframe from the arrays\n",
    "lin_reg_data.columns = ['Coefficient', 'Feature']           # add column names for clarity\n",
    "\n",
    "# Plot\n",
    "ax = sns.barplot(x=\"Coefficient\",                           # add x \n",
    "                 y=\"Feature\",                               # add y\n",
    "                 data=lin_reg_data)                         # specify data\n",
    "\n",
    "ax.set_title(\"OLS Coefficients\")                            # set title\n",
    "plt.show()                                                  # show plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get a sense of how good our model is. We can do this by looking at the difference between the predicted values and the actual values, also called the error.\n",
    "\n",
    "We can see this graphically using a scatter plot.\n",
    "\n",
    "- Call `.predict()` on your linear regression model (`lin_model`), using your validation X, to return a list of predicted number of riders per hour. Save it to a variable `lin_pred`.\n",
    "- Using a scatter plot (`plt.scatter(...)`), plot the predicted values against the actual values (`y_validate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the validation dataset and the trained model, predict the number of riders \n",
    "lin_pred = lin_model.predict(X_validate)\n",
    "\n",
    "# plot the residuals on a scatter plot\n",
    "plt.scatter(y_validate, lin_pred)\n",
    "plt.title('Linear Model (OLS) Predicted v. Actual')\n",
    "plt.xlabel('actual value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What should our scatter plot look like if our model was 100% accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a sense of how well our model is doing by calculating the **root mean squared error**. The root mean squared error (RMSE) represents the average difference between the predicted and the actual values.\n",
    "\n",
    "To get the RMSE:\n",
    "- subtract each predicted value from its corresponding actual value (the errors)\n",
    "- square each error (this prevents negative errors from cancelling positive errors)\n",
    "- average the squared errors\n",
    "- take the square root of the average (this gets the error back in the original units)\n",
    "\n",
    "Write a function `rmse` that calculates the mean squared error of a predicted set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to calculate the root mean squared errror\n",
    "def rmse(pred, actual):\n",
    "    return np.sqrt(np.mean((pred-actual)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the mean squared error for your linear model (`lin_pred`), using the y validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate root mean squared errror\n",
    "rmse(lin_pred,     # specify predicted values \n",
    "     y_validate)   # specify actual values values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note that there is also a built-in function for this in the `sklearn` library.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ridge Regression <a id='section_3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've gone through the process for OLS linear regression, it's easy to do the same for [**Ridge Regression**](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). In this case, the constructor function that makes the model is `Ridge()`.\n",
    "\n",
    "*Tip: the `alpha` parameter controls the regularization strength. Its default is 1 -- try changing it to see what happens.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and fit a Ridge regression model\n",
    "ridge_reg = ...                                                 # create the model\n",
    "ridge_model = ...                                               # fit the model\n",
    "\n",
    "# create a dataframe with the coefficient and feature names for plotting\n",
    "ridge_reg_data = pd.DataFrame([ridge_model.coef_, X.columns]).T\n",
    "ridge_reg_data.columns = ['Coefficient', 'Feature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the coefficients for the Ridge alongside the OLS model to compare them. \n",
    "\n",
    "**QUESTION:** How do they compare to the coefficients for OLS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the figure parameters \n",
    "figure = plt.figure()                            # set the figure space\n",
    "figure.subplots_adjust(wspace = .8, hspace=.5)   # adjust the space in between figures \n",
    "\n",
    "# plot 1\n",
    "# ----------\n",
    "figure.add_subplot(1,   # sets the number of rows\n",
    "                   2,   # sets columns,\n",
    "                   1)   # specifies the following code is for the first plot  \n",
    "\n",
    "# specify barplot for Ridge\n",
    "sns.barplot(...,\n",
    "            ..., \n",
    "            ...).set_title(\"Ridge Coefficients\")\n",
    "\n",
    "# ensure the x-axis is the same on both plots\n",
    "plt.xlim(-2000,6000)\n",
    "\n",
    "# plot 2\n",
    "# ----------\n",
    "figure.add_subplot(1,   # sets the number of rows\n",
    "                   2,   # sets columns,\n",
    "                   2)   # specifies the following code is for the second plot    \n",
    "\n",
    "# specify barplot for OLS\n",
    "sns.barplot(...,\n",
    "            ..., \n",
    "            ...).set_title(\"OLS Coefficients\")\n",
    "\n",
    "# ensure the x-axis is the same on both plots\n",
    "plt.xlim(-2000,6000)\n",
    "\n",
    "# show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your Ridge model to make predictions and visualize the predictions against the actual values. \n",
    "\n",
    "**QUESTION:** How does the RMSE compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions\n",
    "ridge_pred = ...\n",
    "\n",
    "# plot the predictions\n",
    "plt.scatter(...)\n",
    "plt.title('Ridge Model')\n",
    "plt.xlabel('actual values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rmse for the Ridge model\n",
    "rmse(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** ...\n",
    "\n",
    "Note: the documentation for Ridge regression shows it has lots of **hyperparameters**: values we can choose when the model is made. Now that we've tried it using the defaults, look at the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). In a bit, we will try changing some parameters to see if we can get a lower RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LASSO Regression <a id='section_4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll try using [LASSO regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html). The constructor function to make the model is `Lasso()`. \n",
    "\n",
    "You might get a warning message saying the objective did not converge. The model will still work, but to get convergence try increasing the number of iterations (`max_iter=`) when you construct the model. This will generally be an issue if the max number of iterations is less than 15,000 for this model, likely because of the small sample size relative to the number of predictors. Let's set it to 15,000 so we don't have to worry about this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the model\n",
    "lasso_reg = ...    # note the hypterparameter tuning will not converge with max_iter < 15000\n",
    "lasso_model = ...\n",
    "\n",
    "# create a dataframe with the coefficient and feature names for plotting\n",
    "lasso_reg_data = pd.DataFrame([lasso_model.coef_, X.columns]).T\n",
    "lasso_reg_data.columns = ['Coefficient', 'Feature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the coefficients for Ridge and LASSO. \n",
    "\n",
    "**QUESTION:** How do they compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the figure parameters \n",
    "figure = plt.figure()                            # set the figure space\n",
    "figure.subplots_adjust(wspace = .8, hspace=.5)   # adjust the space in between figures \n",
    "\n",
    "# plot 1\n",
    "# ----------\n",
    "figure.add_subplot(1,   # sets the number of rows\n",
    "                   2,   # sets columns,\n",
    "                   1)   # specifies the following code is for the first plot  \n",
    "\n",
    "# specify barplot for Ridge\n",
    "sns.barplot(...,\n",
    "            ..., \n",
    "            ...).set_title(\"Ridge Coefficients\")\n",
    "\n",
    "# ensure the x-axis is the same on both plots\n",
    "plt.xlim(-4000,6000)\n",
    "\n",
    "# plot 2\n",
    "# ----------\n",
    "figure.add_subplot(1,   # sets the number of rows\n",
    "                   2,   # sets columns,\n",
    "                   2)   # specifies the following code is for the second plot    \n",
    "\n",
    "# specify barplot for LASSO \n",
    "sns.barplot(...,\n",
    "            ..., \n",
    "            ...).set_title(\"LASSO Coefficients\")\n",
    "\n",
    "# ensure the x-axis is the same on both plots\n",
    "plt.xlim(-4000,6000)\n",
    "\n",
    "# show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your LASSO model to make predictions and visualize the predictions against the actual values. \n",
    "\n",
    "**QUESTION:** How does the RMSE compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions\n",
    "lasso_pred = ...\n",
    "\n",
    "# plot the predictions\n",
    "plt.scatter(..., ...)\n",
    "\n",
    "# add title and labels\n",
    "plt.title('LASSO Model')\n",
    "plt.xlabel('actual values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rmse for the LASSO model\n",
    "rmse(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: LASSO regression also has many tweakable hyperparameters. See how changing them affects the accuracy!\n",
    "\n",
    "**QUESTION:** How do these three models compare on performance? What sorts of things could we do to improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning <a id='section_5'></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the documentation, you might have noticed that there were a number of arguments that you could supply to each algorithm. How do we decide what the optimal settings are? This process is known as **[hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)**. Hyperparameters are essentially settings that control how the machine learning algorithm learns relationships between features and targets, and are fixed by the analyst. \n",
    "\n",
    "Here, we are going to learn how to accomplish this task using **grid search**. Grid search means we specify a list of hyperparameter specifications that we want to try out, and then we search through all combinations of these specifications. Luckily, Python creates an easy way to do this with its [GridSearchCV](https://scikit-learn.org/stable/modules/grid_search.html) method. This approach combines doing an exhaustive search of hyperparameter values with cross-validation.\n",
    "\n",
    "Look at the documentation for linear regression, and fill in the hyperparameters below. *Note that the `.get_params()` method will show you the hyperparameters available for your different regressor models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see parameters for linear regression\n",
    "lin_reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see parameters for Ridge regression\n",
    "ridge_reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see parameters for Lasso regression\n",
    "lasso_reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Linear regression \n",
    "#-----------\n",
    "\n",
    "# specify the hyperparameters\n",
    "param_grid = {'fit_intercept': [True, False]}          # use dictionary for tuning\n",
    "\n",
    "# execute the grid search\n",
    "lin_grid_reg = GridSearchCV(estimator  = lin_reg,      # model to be tuned\n",
    "                            param_grid = param_grid,   # parameters to be searched as specified above\n",
    "                            cv=3)                      # 3-fold cross-validation to be used during hypertuning\n",
    "\n",
    "# now fit the tuning on the training data\n",
    "lin_grid_reg.fit(X_train, y_train)\n",
    "\n",
    "# select the best performing model and predict with that on validation dataset \n",
    "best_index = np.argmax(lin_grid_reg.cv_results_[\"mean_test_score\"])  # find the best performing model\n",
    "best_lin_pred = lin_grid_reg.best_estimator_.predict(X_validate)     # find best estimator and predict on validation\n",
    "\n",
    "# print the results  \n",
    "print(lin_grid_reg.cv_results_[\"params\"][best_index])\n",
    "print('Best CV R^2:', max(lin_grid_reg.cv_results_[\"mean_test_score\"]))\n",
    "print('Validation R^2:', lin_grid_reg.score(X_validate, y_validate))\n",
    "print('Validation RMSE', rmse(best_lin_pred, y_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement a grid search for both Ridge and LASSO. In particular, make sure to search across a variety of alpha values in addition to varying other hyperparameters.\n",
    "\n",
    "*Recall: `GridSearchCV` takes two arguments: a model and a dict of parameter variations (`param_grid`).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# RIDGE \n",
    "#-----------\n",
    "# We will run 9*2*4 = 72 tests, each with 3-fold cross validation\n",
    "\n",
    "\n",
    "# specify the hyperparameters\n",
    "param_grid = {'alpha': np.arange(.1, 1, .1),\n",
    "              'fit_intercept': [True, False],\n",
    "              'solver': ['auto', 'svd', 'cholesky', 'lsqr']}\n",
    "\n",
    "# execute the grid search\n",
    "ridge_grid_reg = GridSearchCV(...,   # model to be tuned\n",
    "                              ...,   # parameters to be searched as specified above\n",
    "                              cv=3)  # 3-fold cross-validation to be used during hypertuning\n",
    "\n",
    "# fit the tuning on the training data\n",
    "...\n",
    "\n",
    "# select the best performing model and predict with that on validation dataset \n",
    "best_index = ...\n",
    "best_ridge_pred = ...\n",
    "\n",
    "# Print the best parameters, CV, validation r2, and validation RMSE\n",
    "...\n",
    "...\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# LASSO \n",
    "#-----------\n",
    "\n",
    "# specify the hyperparameters\n",
    "param_grid = {'alpha': np.arange(.1, 1, .1),     # model to be tuned\n",
    "              'fit_intercept': [True, False],    # parameters to be searched as specified above\n",
    "              'selection': ['cyclic', 'random']} # 3-fold cross-validation to be used during hypertuning\n",
    "\n",
    "# execute the grid search\n",
    "lasso_grid_reg = GridSearchCV(..., ..., cv=3)\n",
    "\n",
    "# now fit the tuning on the training data\n",
    "...\n",
    "\n",
    "# select the best performing model and predict with that on validation dataset \n",
    "best_index = ...\n",
    "best_lasso_pred = ...\n",
    "\n",
    "# Print the best parameters, CV, validation r2, and validation RMSE\n",
    "...\n",
    "...\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Choosing a model <a id='section_6'></a>\n",
    "---\n",
    "### Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we'd want to choose our best model (of the threelinear, Ridge, and LASSO) based on the best hyperparameter values and try it on the test set. In what follows, we'll test our best model of each (linear, Ridge, and LASSO) just so we can see what it will look like. Unless model selection among a few options is the goal of your work, you will just select the best type of model and apply that to the test data. \n",
    "\n",
    "How well do they do?\n",
    "\n",
    "Make sure to look at the documentation for [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and see which methods are available.\n",
    "\n",
    "*Tip: first, apply the `.best_estimator_.predict()` method to one of your GridSearchCV objects--`lin_grid_reg`, `ridge_grid_reg`, or `lasso_grid_reg`--depending on which one was most succesful.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Linear model\n",
    "\n",
    "# pick the best estimator and predict on test\n",
    "best_pred = ...\n",
    "\n",
    "# print various results\n",
    "print('Best CV R^2:', max(....cv_results_[\"mean_test_score\"]))\n",
    "print('Test R^2:', ....score(X_test, y_test))\n",
    "print('Test RMSE', rmse(..., ...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Ridge model\n",
    "\n",
    "# pick the best estimator and predict on test\n",
    "best_pred = ...\n",
    "\n",
    "# print various results\n",
    "print('Best CV R^2:', max(....cv_results_[\"mean_test_score\"]))\n",
    "print('Test R^2:', ....score(X_test, y_test))\n",
    "print('Test RMSE', rmse(..., ...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best LASSO model\n",
    "\n",
    "# pick the best estimator and predict on test\n",
    "best_pred = ...\n",
    "\n",
    "# print various results\n",
    "print('Best CV R^2:', max(....cv_results_[\"mean_test_score\"]))\n",
    "print('Test R^2:', ....score(X_test, y_test))\n",
    "print('Test RMSE', rmse(..., ...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS:** \n",
    "1. How do the RMSEs for the test data compare to those for the training and validation data? Why? \n",
    "2. Did the model that performed best on the training and validation set also do best on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming up this semester: how to select your models, model parameters, and features to get the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Authored by Aniket Kesari. Materials borrowed from notebook developed by Keeley Takimoto for LS123: Data, Prediction, and Law. Updated by Tom van Nuenen in 2022 and again by Kasey Zapatka in 2023."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
