{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science]\n",
    "## 3-2 Tree-Based Methods - Student Version\n",
    "\n",
    "In this lab, we will explore decision trees and their extensions. In the next lab, we will introduce ensemble machine learning, which involves combining several machine learning algorithms together to create a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environment\n",
    "Remember to always activate your virtual environment first before you install packages or run a notebook! This helps to prevent conflicts between dependencies across different projects and ensures that you are using the correct versions of packages. You must have created anaconda virtual enviornment in the `Anaconda Installation` lab. If you have not or want to create a new virtual environment, follow the instruction in the `Anaconda Installation` lab. \n",
    "\n",
    "<br>\n",
    "\n",
    "If you have already created a virtual enviornment, you can run the following command to activate it: \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate <virtual_env_name>`\n",
    "\n",
    "<br>\n",
    "\n",
    "For example, if your virtual environment was named as CSS, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate CSS`\n",
    "\n",
    "<br>\n",
    "\n",
    "To deactivate your virtual environment after you are done working with the lab, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda deactivate`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We're going to use our [Census Income dataset](https://archive.ics.uci.edu/dataset/20/census+income) again for this lab. Let's load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# settings\n",
    "%matplotlib inline\n",
    "#sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of column names, found in \"adult.names\"\n",
    "col_names = ['age', \n",
    "             'workclass', \n",
    "             'fnlwgt',\n",
    "             'education', \n",
    "             'education-num',\n",
    "             'marital-status', \n",
    "             'occupation', \n",
    "             'relationship', \n",
    "             'race', \n",
    "             'sex', \n",
    "             'capital-gain',\n",
    "             'capital-loss', \n",
    "             'hours-per-week',\n",
    "             'native-country', \n",
    "             'income-bracket']\n",
    "\n",
    "# Read table from the data folder\n",
    "census = pd.read_table(\"../../data/adult.data\", \n",
    "                       sep = ',', \n",
    "                       names = ...)\n",
    "census.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we need to preprocess the data to binarize the target and dummify our categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "# ----------\n",
    "# initialize binarizer function and store a binary version of the outcome variable as \"y\"\n",
    "lb_style = ...\n",
    "y = census['income-bracket-binary'] = ...\n",
    "\n",
    "# Features \n",
    "# ----------\n",
    "# drop 3 variables: income-bracket, fnlwgt, and income-bracket-binary\n",
    "X = census.drop(..., \n",
    "                axis = ...)\n",
    "# get dummies\n",
    "X = ...\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we will look at is the decision tree. Using the [`tree.DecisionTreeClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) method, let's implement a cross-validation approach to predicting income. We will initialize the model with the standard configurations from the Classification lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Decision Tree Classifier\n",
    "# ----------\n",
    "dt_classifier = tree.DecisionTreeClassifier(\n",
    "                       criterion='gini',              # or 'entropy' for information gain\n",
    "                       splitter='best',               # or 'random' for random best split\n",
    "                       max_depth=None,                # set how deep tree nodes can go\n",
    "                       min_samples_split=2,           # samples (observations) needed to split node\n",
    "                       min_samples_leaf=1,            # samples (observations) needed for a leaf\n",
    "                       min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                       max_features=None,             # number of features to look for when splitting\n",
    "                       max_leaf_nodes=None,           # max nodes\n",
    "                       min_impurity_decrease=1e-07,   # early stopping\n",
    "                       random_state = 10)             #random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_score returns the accuracy score by default but you can change this with the \"scoring\" argument\n",
    "scores = cross_val_score(...,             # specify estimator \n",
    "                         X,               # specify X\n",
    "                         y,               # specify y\n",
    "                         ...)             # specify 5 cross-validation folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view accuracy\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean accuracy score from the results of cross validation\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".82 accuracy, not bad! We can also visualize the decision tree to see how it made its splits. Note we limit the max depth to 4 so that the code runs quickly, but in practice you might want to visualize the entire tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape of the data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit to data\n",
    "# ----------\n",
    "dt_classifier.fit(X, y)\n",
    "\n",
    "# set column names as list\n",
    "column_names = X.columns.tolist()\n",
    "\n",
    "# plot the figure\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "_ = tree.plot_tree(dt_classifier,  \n",
    "                   feature_names=column_names,      # make sure its a list\n",
    "                   class_names=[\"<=50k\", \">50k\"],   # specify class names\n",
    "                   filled=True,                     # paint nodes to indicate majority class \n",
    "                   fontsize = 15,                   # set fontsize\n",
    "                   max_depth = 3)                   # set max depth of tree to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the .max_depth attribute to check out the depth of our entire tree\n",
    "dt_classifier.tree_.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remind ourselves how many samples in our negative class\n",
    "np.count_nonzero(y==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the samples after root node\n",
    "X['marital-status_ Married-civ-spouse'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the most informative features\n",
    "# ----------\n",
    "importances = pd.DataFrame({'feature':X.columns,'importance':np.round(dt_classifier.feature_importances_,3)})\n",
    "importances = importances.sort_values('importance',ascending=False)\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Authored by Aniket Kesari. Minor edits by Tom van Nuenen 2022 and Kasey Zapatka in 2023."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
