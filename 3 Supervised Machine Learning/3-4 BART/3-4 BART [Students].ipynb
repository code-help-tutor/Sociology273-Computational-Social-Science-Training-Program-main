{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c15ff7bd",
   "metadata": {},
   "source": [
    "# [Computational Social Science] \n",
    "## 3-4 BART - Students\n",
    "\n",
    "In this lab, we will **Bayesian Additive Regression Trees (BART),** which is a non-linear and non-parametric Bayesian statistical modeling technique. BART can  used for regression and classification problems and it has been shown to provide flexible fits to complex datasets that often outperform traditional modeling techniques like linear regression, classification, and even some regression trees.\n",
    "\n",
    "BART models data as a sum of decision trees where each tree is constrained by a prior to be a weak learner, contributing a small amount to the overall prediction. This setup helps avoid overfitting, which can be an issue  with methods that use decision trees, such as random forests.\n",
    "\n",
    "In more technical terms, given an outcome variable Y and a vector of predictor variables X1, X2, ..., Xp, BART assumes the following relationship:\n",
    "\n",
    "`Y = f(X1, X2, ..., Xp) + e`\n",
    "\n",
    "where f(X1, X2, ..., Xp) is some unknown function and *e* is a normally distributed error term. BART approximates the unknown function *f* using an additive model of binary trees. The \"Bayesian\" part of BART comes in in how it uses Bayesian statistical principals to learn from the data and make predictions, while also providing a way to do statistical inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46500cc6",
   "metadata": {},
   "source": [
    "## Virtual Environment\n",
    "Remember to always activate your virtual environment first before you install packages or run a notebook! This helps to prevent conflicts between dependencies across different projects and ensures that you are using the correct versions of packages. You must have created anaconda virtual enviornment in the `Anaconda Installation` lab. If you have not or want to create a new virtual environment, follow the instruction in the `Anaconda Installation` lab. \n",
    "\n",
    "<br>\n",
    "\n",
    "If you have already created a virtual enviornment, you can run the following command to activate it: \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate <virtual_env_name>`\n",
    "\n",
    "<br>\n",
    "\n",
    "For example, if your virtual environment was named as CSS, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate CSS`\n",
    "\n",
    "<br>\n",
    "\n",
    "To deactivate your virtual environment after you are done working with the lab, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda deactivate`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba8e2f",
   "metadata": {},
   "source": [
    "## Libraries and environment settings\n",
    "\n",
    "\n",
    "Let's get started by importing some libraries and specfying some settings for our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8736cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment & install if pymc and pymc_bart is not already installed in your machine\n",
    "#!pip install pymc\n",
    "#!pip install pymc_bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# ----------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz as az                 # ArviZ is a Python package for exploratory analysis of Bayesian models\n",
    "import pymc as pm                  # PyMC (formerly PyMC3) is a Python package for Bayesian statistical modeling\n",
    "import pymc_bart as pmb            # PyMC-BART extends PyMC \n",
    "import warnings\n",
    "\n",
    "# import sub-modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# run this command to suppress sampling notifications\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Since the library we use for BART modeling (PyMC) might not work in older versions, let's print our the version \n",
    "print(f\"Running on PyMC v{pm.__version__}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32a4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "# ----------\n",
    "# this is a slightly different but more efficient way to see a random seed bc you can call \n",
    "# the object \"random_seed\" later in the script but only change it once at the top of the script\n",
    "random_seed = 300 \n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# settings for ArviZ (which is specifically used to visualize Bayesian models)\n",
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd380f6e",
   "metadata": {},
   "source": [
    "### Data and pre-processing\n",
    "\n",
    "In this lab, we will first use coal mining disaster dataset. This is a popular dataset in the [PYMC library documentation](https://www.pymc.io/projects/bart/en/latest/examples/BART_introduction.html) used to illustrate some basic principals of BART modeling. We will discretize the dataset by building a histogram and use center of bins as x variable and counts per bin as y variable. \n",
    "\n",
    "The file \"coal.csv\" comes when you load the `PyMC` library. The `numpy.loadtxt()` function expects the file to be a text file with numerical data, and will load the data into a NumPy array called `coal.` \n",
    "\n",
    "*Note that the file is a .csv file, which stands for comma delimited value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b01b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the coal mining disaster data\n",
    "coal = np.loadtxt(pm.get_data(\"coal.csv\")) \n",
    "\n",
    "# Convert the data to a pandas DataFrame so we can take a quick look\n",
    "coal_data = pd.DataFrame(coal)\n",
    "\n",
    "# Now `coal_data` is a pandas DataFrame containing the coal mining disaster data.\n",
    "coal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f1901",
   "metadata": {},
   "source": [
    "Notice this is a bit different than with other datasets we have worked with because the `coal` dataframe only has one column, which corresponds to dates of coal mining disaters. \n",
    "\n",
    "So, to get some predcitors we can use, we are going to discretize the data, just as if we were building a histogram. Specifically, we are going to use the centers of the bins as the **X** variable and the counts per bin as the **y** variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Discretize data\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# STEP 1:\n",
    "# --------\n",
    "# Let's first calculate the range of the data. \n",
    "# If the coal dataset represents years, this would give the number of years the data spans.\n",
    "years = int(coal.max() - coal.min()) \n",
    "\n",
    "\n",
    "# STEP 2:\n",
    "# --------\n",
    "# Let's calculate the number of bins per year we want to used to divide our data. \n",
    "# The '//' calculates the number of bins for the histogram by performing floor division,\n",
    "# where the  \"//\" operator divides and rounds down to the nearest whole number.\n",
    "bins = years // 4  # four corresponds to the number of bins we want for our illustration\n",
    "\n",
    "# STEP 3:\n",
    "# --------\n",
    "# This line creates an object \"hist\" that is an array with the number of\n",
    "# elements in each bin, and \"x_edges\" is an array with the edge values of those bins.\n",
    "# compute the location of the centers of the discretized data\n",
    "hist, x_edges = np.histogram(coal, \n",
    "                             bins=bins) \n",
    "\n",
    "# STEP 4:\n",
    "# --------\n",
    "# This step calculates the centers of each bin by adding half the bin width \n",
    "# to the left edge of each bin (x_edges[:-1]). This is done to represent each \n",
    "# bin by its center point rather than its edges.\n",
    "x_centers = x_edges[:-1] + (x_edges[1] - x_edges[0]) / 2\n",
    "\n",
    "# STEP 5:\n",
    "# --------\n",
    "# This step converts \"x_centers\" object into a 2D array with one column. \n",
    "# The None indexing is used to add an extra dimension. This is done because the BART  \n",
    "# model expects 2D input.\n",
    "x_data = x_centers[:, None] \n",
    "\n",
    "\n",
    "# STEP 6:\n",
    "# --------\n",
    "# just renames hist to y_data. This data represents the counts in each bin \n",
    "# or the number of disasters per year.\n",
    "y_data = hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6acb189",
   "metadata": {},
   "source": [
    "### Fitting a model\n",
    "\n",
    "Now that we have a predictor (y_data), we can fit that to our outcome. A few things to note. The PyMC library has a different syntax for fitting a model. We'll use the `with pm.model() as model_coal:` as the syntax to run the model. Keep in mind that everything that happens within the with block pertains to this \"model_coal\".\n",
    "\n",
    "**Setting `m` parameter:**\n",
    "One important parameter of these models is `m`, which is the number of trees we are going to sum over. A low number of trees like 20 could be good enough for simple models like this and could also work very good as a quick approximation for more complex models. This is particularly helpful during early stage modeling when we may want to try a few things as quickly as possible in order to better grasp which model may be best for our problem. In those cases once we have more certainty about the model(s) we really like we can improve the approximation by increasing `m,` in the literature is common to find reports of good results with numbers like 50, 100 or 200.\n",
    "\n",
    "**Transformation of the outcome:**\n",
    "Lastly, since possible BART variable can sample over a range between negative and positive infinity, we may need to transform their values based on the model we are used for prediction. In this illustration, we are using a model that relies on a Poisson distribution, which expects values that go from 0 to positive infinity. This does not necesssarily mean we make transformations for every model. See this [documentation](https://www.pymc.io/projects/bart/en/latest/examples/BART_introduction.html) for more explanation on this matter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8f3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Fit BART Model\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Create a new probabilistic model\n",
    "with pm.Model() as model_coal: \n",
    "    \n",
    "    # Step 1: fit the model and store the output\n",
    "    # ----------\n",
    "    μ_ = pmb.BART(\"μ_\",             # store model output in \"μ_\" object\n",
    "                  X=x_data,         # setting predictor data\n",
    "                  Y=np.log(y_data), # setting outcome (normalizing by taking the logarithm)\n",
    "                  m=20)             # This parameter defines the number of trees to used to sum over\n",
    "    \n",
    "    # Step 2: exponentiate the stored outcome \n",
    "    # ----------\n",
    "    # This line is defining a deterministic variable μ, which is the exponential of μ_. \n",
    "    # This is done because μ_ is on the log scale due to the np.log(y_data) used in the BART model.\n",
    "    μ = pm.Deterministic(\"μ\", \n",
    "                         pm.math.exp(μ_)) \n",
    "\n",
    "    # Step 3: compare model with Poisson to get predictions\n",
    "    # ----------\n",
    "    y_pred = pm.Poisson(\"y_pred\",        # store model output in \"y_pred\" object\n",
    "                        mu=μ,            # parameter for the Poisson distribution representing the mean\n",
    "                        observed=y_data) # observed data that the model is conditioned \n",
    "    \n",
    "    # Step 4: sample the posterior distribution of the model\n",
    "    # ----------\n",
    "    idata_coal = pm.sample(random_seed=random_seed) # sample from the posterior distribution of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df562ce",
   "metadata": {},
   "source": [
    "### Visualizing our model fit\n",
    "\n",
    "The figure below visualizes our results from the model. The white line in the plot above shows the mean rate of accidents over 4000 posterior draws sumed over 20 trees. The darker orange band represents the HDI 50% and the lighter one the 94%. We can see a rapid decrease of coal accidents between 1880 and 1900. \n",
    "\n",
    "**Highest Density Interval (HDI):** is an interval that contains the range of values for a parameter where the probability density is highest. So, it is similar to a confidence interval but instead of giving you a range of values that the parameter could fall into with a certain probability, it gives you a range of values that the parameter is most likely to fall into based on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba215680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Visualize fit\n",
    "# ---------------------------------------------------------\n",
    "# The underscore (_) is a common convention in Python for a variable name that we don't plan to use.\n",
    "\n",
    "# plot settings\n",
    "_, ax = plt.subplots(figsize=(10, 6)) \n",
    "\n",
    "# sample posterior distribution and calcualte mean\n",
    "rates = idata_coal.posterior[\"μ\"] / 4\n",
    "rate_mean = rates.mean(dim=[\"draw\", \"chain\"])\n",
    "\n",
    "# plot specifications\n",
    "ax.plot(x_centers, rate_mean, \"w\", lw=3)\n",
    "ax.plot(x_centers, y_data / 4, \"k.\")\n",
    "az.plot_hdi(x_centers, rates, smooth=False)\n",
    "az.plot_hdi(x_centers, rates, hdi_prob=0.5, smooth=False, plot_kwargs={\"alpha\": 0})\n",
    "ax.plot(coal, np.zeros_like(coal) - 0.5, \"k|\")\n",
    "ax.set_xlabel(\"years\")\n",
    "ax.set_ylabel(\"rate\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e899f6",
   "metadata": {},
   "source": [
    "####  Viualizing how an ensemble of BART trees approximates a smooth function\n",
    "\n",
    "Whereas the previous illustration visuzlized the mean over 4000 posterior draws over the sum of 20 trees, the  following figure shows two samples from the posterior distributions. Although these functions are not smooth, they illustrate how using a combination of regression trees is **a way to represent stepwise function to approximate a distrubution.** Stepwise functions are valuable when a quantity changes abruptly at specific points or intervals. However, when summing stepwise functions, the result is another stepwise function. In BART, we make the assumption that a stepwise function is a suitable approximation for our problem. \n",
    "\n",
    "This is often the case, because we summing over many trees (e.g., 50, 100, or 200) and average over the posterior distribution. Together, these operations make the steps appear smoother, even when the underlying function is not truly smooth, as in the case of Gaussian processes or splines.A nice theoretical result, tells us that in the limit of **m -> ∞** . The BART prior converges to a [nowheredifferentiable](https://en.wikipedia.org/wiki/Weierstrass_function) Gaussian process.\n",
    "\n",
    "The following figure shows two samples of μ from the posterior.\n",
    "\n",
    "**QUESTION:** How well do these two samples approximate the the mean we saw above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b017d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple illustration of two samples from the posterior of μ\n",
    "# ----------\n",
    "plt.step(x_data, \n",
    "         rates.sel(chain=0, draw=[3, 10]).T);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba8269",
   "metadata": {},
   "source": [
    "**ANSWER:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a87f07",
   "metadata": {},
   "source": [
    "**QUESTION:** How well would 10 samples approximate the the mean we saw above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we drew 10 samples?\n",
    "# ----------\n",
    "plt.step(x_data, \n",
    "         rates.sel(chain=0, draw=range(10)).T);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc03b9c",
   "metadata": {},
   "source": [
    "**ANSWER:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d2037f",
   "metadata": {},
   "source": [
    "**QUESTION:** How about if we draw 50?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3135d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we drew 50 samples and took the average?\n",
    "# ----------\n",
    "\n",
    "# Calculate the mean of the draws along the draw axis (axis=1)\n",
    "mean_draws = rates.sel(chain=0, \n",
    "                       draw=range(50)).mean(dim='draw', skipna=True)\n",
    "\n",
    "# Plot the step plot\n",
    "plt.step(x_data, rates.sel(chain=0, \n",
    "                           draw=range(50)).T)\n",
    "\n",
    "# Add the average line\n",
    "plt.plot(x_data, \n",
    "         mean_draws, \n",
    "         color='#8B0000',\n",
    "         linewidth=5, \n",
    "         label='Mean')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424853cd",
   "metadata": {},
   "source": [
    "**ANSWER:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6f32a",
   "metadata": {},
   "source": [
    "## Biking with BART\n",
    "\n",
    "Now let's explore BART using four covariate using a subset of bike sharing dataset.\n",
    "We will use the following variables: number of bike rental in a city, the hour of the day, the temperature, the humidity, and whether it is a weekday or weekend. \n",
    "\n",
    "To explore other features offered by BART in PyMC. We are now going to move on to a different example. In this example we have data about the  number of bikes rental in a city, and we have chosen four covariates; the hour of the day, the temperature, the humidity and whether is a workingday or a weekend. This dataset is a subset of the [bike_sharing_dataset](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset). \n",
    "\n",
    "*Note: Unlike in the past illustration where we used these data, this is a subset of the of the records and cases.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0149f903",
   "metadata": {},
   "source": [
    "### Data and pre-processing\n",
    "\n",
    "Load the dataset and take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load bike data\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# load data\n",
    "# ---------\n",
    "bikes = pd.read_csv(pm.get_data(\"bikes.csv\"))  # the file bikes.csv is included in pymc installation\n",
    "bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d554ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify feature list for subsetting\n",
    "# ---------\n",
    "features = [\n",
    "            \"hour\", \n",
    "            \"temperature\",\n",
    "            \"humidity\",\n",
    "            \"workingday\"\n",
    "           ]\n",
    "\n",
    "# partition data\n",
    "# ---------\n",
    "X = bikes[features]\n",
    "y = bikes[\"count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4dfad2",
   "metadata": {},
   "source": [
    "### Fit model\n",
    "\n",
    "Below, we follow a similar workflow to above. The main difference is that we compare our distribution to a negative binomial instead of a Possion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Fit BART Model\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Create a new probabilistic model\n",
    "with pm.Model() as model_bikes:\n",
    "    \n",
    "    # Step 1: Define the prior distribution to be used after model fit\n",
    "    # ----------\n",
    "    α = pm.Exponential(\"α\", \n",
    "                       1)\n",
    "    \n",
    "    # Step 2: fit the model and store the output\n",
    "    # ----------\n",
    "    μ = pmb.BART(\"μ\",             # store model output in \"μ_\" object\n",
    "                 X,               # setting predictor data\n",
    "                 np.log(y),       # setting outcome (normalizing by taking the logarithm)\n",
    "                 m=50)            # This parameter defines the number of trees to used to sum over\n",
    "    \n",
    "    # Step 3: compare model with Negative Binomial to get predictions\n",
    "    # ----------\n",
    "    y_pred = pm.NegativeBinomial(\"y\",          # store model output in \"y_pred\" object\n",
    "                            mu=pm.math.exp(μ), # parameter for the Poisson distribution representing the mean\n",
    "                            alpha=α,           # define the prior distribution\n",
    "                            observed=y)        # observed data that the model is conditioned \n",
    "     \n",
    "    # Step 4: sample the posterior distribution of the model\n",
    "    # ----------\n",
    "    idata_bikes = pm.sample(compute_convergence_checks=False, \n",
    "                            random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47eb26d",
   "metadata": {},
   "source": [
    "### Convergence diagnostics\n",
    "\n",
    "To check sampling convergence of BART models we recommend a 2 step approach. \n",
    "\n",
    "* For the non-BART variables (like $\\alpha$ in `model_bikes`) we follow the standard recommendations, like checking R-hat (<= 1.01), and ESS (< 100x number of chains) numerical diagnostics as well as using trace plots or rankplots\n",
    "* For the BART variables we recommend using the `pmb.plot_convergence` function. \n",
    "\n",
    "**Which diagnostics to check:**\n",
    "Some argue in the BART literature that the diagnostics of the BART variables are less important than the diagnostics of the non-BART variables. The main argument is that the individual estimates of the latent variables are of no direct interest, and instead we should only care about how well we are estimating the whole function/regression.\n",
    "\n",
    "However, the library mainatiners argue for checking the convergence of BART variables is an important part of the Bayesian workflow. The main reason to use `pmb.plot_convergence` is that usually the BART variable will be a large vector (we estimate a distribution per observation) and thus we will need to check a large number of diagnostics. \n",
    "\n",
    "Additionally, the R-hat threshold of 1.01 is not a hard threshold, this value was chosen assuming one or a few R-hats are examined (and chains are long enough to accurately estimate their autocorrelation), and if we observed a large number of R-hat a few of them are expected to be larger than the 1.01 threshold (or whatever threshold we pick) even if there is nothing wrong with our inference. For that reason, a fair analysis should include a multiple comparison adjustment, and that's what `pmb.plot_convergence` does automatically for you. \n",
    "\n",
    "See this [introductory tutorial](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#pymc-overview) for a more in-depth discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking non-BART models\n",
    "# ----------\n",
    "az.plot_trace(idata_bikes, \n",
    "              var_names=[\"α\"], \n",
    "              kind=\"rank_bars\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5b8d1",
   "metadata": {},
   "source": [
    "**So, how to read its output?:** We have two panels one for ESS and one for the R-hat. The blue line is the empirical cumulative distribution for those values, for the ESS we want the entire curve above the dashed line, and for R-hat we want the curve to be entirely below the dashed line. In the previous figure, we can see that we barely make it for the ESS and for the R-hat we have some values above the threshold. Are our results useless? Most likely not, but to be sure we may want to take a few more draws. \n",
    "\n",
    "**ESS:** Explained sum of squares\n",
    "\n",
    "**R-hat:** Predicted fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a16ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking BART variables \n",
    "# ----------\n",
    "pmb.plot_convergence(idata_bikes, \n",
    "                     var_name=\"μ\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86a951",
   "metadata": {},
   "source": [
    "### Partial dependence plots\n",
    "\n",
    "To help us interpret the results of our model we can use **partial dependence plots** (We covered these in the Lundberg et. al. paper). This is a type of plot that shows the *marginal effect that one covariate has on the predicted variable*. That is, what is the effect that a covariate $X_i$ has of $Y$ while we average over all the other covariates ($X_j, \\forall j \\not = i$). \n",
    "\n",
    "Remeber, this type of plot are not exclusive of BART, but are often used in the BART literature. PyMC-BART provides an utility function to make this plot from the inference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e2431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# partial dependence plot\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# explore plot_pdp function futher at https://github.com/pymc-devs/pymc-bart/blob/main/pymc_bart/utils.py\n",
    "_ = pmb.plot_pdp(μ,            # specify the model\n",
    "                 X=X,          # specify the features dataset\n",
    "                 Y=y,          # specify the labels dataset\n",
    "                 grid=(2, 2),  # set the number of grids\n",
    "                 func=np.exp); # specify an exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857bef6f",
   "metadata": {},
   "source": [
    "From this plot we can see the main effect of each covariate on the predicted value. This is very useful as we can recover complex relationship beyond monotonic increasing or decreasing effects (more informative than interpreting a simple regression output). For example, for the `hour` covariate we can see two peaks around 8 and and 17 hours (5pm) and a minimum at midnight (0).\n",
    "\n",
    "When interpreting partial dependence plots we should be careful about the assumptions in this plot. **First,** we are assuming variables are independent. For example when computing the effect of `hour` we have to marginalize the effect of `temperature` and this means that to compute the partial dependence value at `hour=0` we are including all observed values of temperature, and this may include temperatures that are actually not observed at midnight, given that lower temperatures are more likely than higher ones. \n",
    "\n",
    "**Second,** the plots are only showing us averages, so if for a covariate half the values are positively associated with predicted variable and the other half negatively associated. The partial dependence plot will be flat as their contributions will cancel each other out. This is a problem that can be solved by using individual conditional expectation plots `pmb.plot_dependence(..., kind=\"ice\")`. **Notice that all of these assumptions are assumptions of the partial dependence plot, not of our model!** In fact BART can easily accommodate interaction of variables. Although the prior in BART regularizes high order interactions. \n",
    "\n",
    "For more on interpreting Machine Learning model you could check the \"[Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/)\" (chapter 8).\n",
    "\n",
    "**Finally,** like with other regression methods we should be careful that the effects we are seeing on individual variables are conditional on the inclusion of the other variables. So for example, while `humidity` seems to be mostly flat, meaning that this covariate has an small effect of the number of used bikes. This could be the case because `humidity` and `temperature` are correlated to some extend and once we include `temperature` in our model `humidity` does not provide too much extra information. \n",
    "\n",
    "For example, try fitting the model again but this time with `humidity` as the single covariate and then fitting the model again with `hour` as a single covariate. You should see that the result for this single-variate models will be very similar to the previous figure for the `hour` covariate, but less similar for the `humidity` covariate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998ef64",
   "metadata": {},
   "source": [
    "### Variable importance\n",
    "\n",
    "As we saw in the previous section a partial dependence plot can visualize give us an idea of how much each covariable contributes to the predicted outcome. But BART itself leads to a simple heuristic to estimate variable importance. That is simple count how many times a variable is included in all the regression trees. The intuition is that if a variable is important they it should appears more often in the fitted trees that less important variables. While this heuristic seems to provide reasonable results in practice, there is not too much theory justifying this procedure, at least not yet.\n",
    "\n",
    "The following plot shows the relative importance in a scale from 0 to 1 (less to more importance) and the sum of the individual importance is 1. At least in this case, the relative importance qualitative agrees with the partial dependence plot in that `hour` seems to be the most important variable. After we begin to lose relative importance.\n",
    "\n",
    "Additionally, PyMC-BART provides a novel method to assess the variable importance. You can see an example in the bottom panel. On the x-axis we have the number of covariables and on the y-axis the square of the Pearson correlation coefficient between the predictions made for the full-model (all variables included) and the restricted-models, those with only a subset of the variables. The components are included following the relative variable importance order, as show in the top panel. Thus, in this example \"number of covariables\" is 1 `hour`, 2  `hour` and `temperature`, 3 `hour`, `temperature`and `humidity`. Finally, 4 means `hour`, `temperature`, `humidity`, `workingday`, i.e., the full model. Hence, from the next figure we can see that even a model with a single component, `hour`, is very close to the full model. Even more, the model with two components `hour`, and `temperature` is on average indistinguishable from the full model. The error bars represent the 94 \\% HDI from the posterior predictive distribution. It is important to notice that to compute these correlations we do not resample the models, instead the predictions of the restricted-models are approximated by *prunning* variables from the full-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4042a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# variable importance \n",
    "# ---------------------------------------------------------\n",
    "\n",
    "pmb.plot_variable_importance(idata_bikes,  # set the data frame\n",
    "                             μ,            # set \n",
    "                             X,            # set the feastures to model\n",
    "                             samples=100); # how many samples do we want to draw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcac7a9",
   "metadata": {},
   "source": [
    "### Out-of-Sample Predictions\n",
    "\n",
    "In this section we want to show how to do out-of-sample predictions with BART. We are going to use the same dataset as before, but this time we are going to split the data into a training and a test set. We are going to use the training set to fit the model and the test set to evaluate the model. We can then compare the two.\n",
    "\n",
    "Let's start by randomly spliting the data into a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# split into training and test data\n",
    "# ---------------------------------------------------------\n",
    "# split into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c67d3",
   "metadata": {},
   "source": [
    "Now, we fit the same model as above but this time using a *shared variable* for the covariatates so that we can then replace them to generate the out of sample predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa266f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Fit BART Model\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Create a new probabilistic model\n",
    "with pm.Model() as model_oos_regression:\n",
    "    \n",
    "        \n",
    "    # Step 1: Define the predictor variables (X) as mutable data, this allows them to updated\n",
    "    # ----------\n",
    "    X = pm.MutableData(\"X\", \n",
    "                       X_train)\n",
    "    \n",
    "        \n",
    "    # Step 2: Define the observed data that will be used in the model\n",
    "    # ----------\n",
    "    y = y_train\n",
    "    \n",
    "        \n",
    "    # Step 3: Define the prior distribution for the parameter α\n",
    "    # ----------\n",
    "    α = pm.Exponential(\"α\",\n",
    "                       1)\n",
    "    \n",
    "        \n",
    "    # Step 4: Fit the BART model (μ) using predictor variables (X) and log-transformed outcome (np.log(y))\n",
    "    # ----------\n",
    "    μ = pmb.BART(\"μ\",       # store model output in \"μ_\" object\n",
    "                 X,         # setting predictor data\n",
    "                 np.log(y)) # setting outcome (normalizing by taking the logarithm)\n",
    "    \n",
    "        \n",
    "    # Step 5: Define the likelihood distribution using a Negative Binomial distribution\n",
    "    # ----------\n",
    "    y_pred = pm.NegativeBinomial(\"y\", \n",
    "                                 mu=pm.math.exp(μ), \n",
    "                                 alpha=α,\n",
    "                                 observed=y, \n",
    "                                 shape=μ.shape)\n",
    "    \n",
    "    # Step 6: Sample from the posterior distribution of the model\n",
    "    # ----------\n",
    "    idata_oos_regression = pm.sample(random_seed=random_seed)\n",
    "    \n",
    "        \n",
    "    # Step 7: Generate posterior predictive samples for training data\n",
    "    # ----------\n",
    "    posterior_predictive_oos_regression_train = pm.sample_posterior_predictive(\n",
    "        trace=idata_oos_regression, \n",
    "        random_seed=random_seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a84f37",
   "metadata": {},
   "source": [
    "Next, we replace the data in the model and sample from the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# make posterior predictions\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Create a new probabilistic model\n",
    "with model_oos_regression:\n",
    "\n",
    "    # Step 1: update value of the predictor variable X with the test data \n",
    "    # ----------\n",
    "    X.set_value(X_test)\n",
    "    \n",
    "    # Step 2: generates posterior predictive samples for the test data\n",
    "    # ----------\n",
    "    posterior_predictive_oos_regression_test = pm.sample_posterior_predictive(\n",
    "        trace=idata_oos_regression, random_seed=random_seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599975d1",
   "metadata": {},
   "source": [
    "Finally, we can compare the posterior predictive distribution with the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bba1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# check posterior predictions on training and test data\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# specify plot parameters\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=2, \n",
    "    ncols=1, \n",
    "    figsize=(8, 7),\n",
    "    sharex=True, \n",
    "    sharey=True, \n",
    "    layout=\"constrained\"\n",
    ")\n",
    "\n",
    "# plot training posterior\n",
    "# -----------\n",
    "az.plot_ppc(\n",
    "    data=posterior_predictive_oos_regression_train, \n",
    "    kind=\"cumulative\",\n",
    "    observed_rug=True, \n",
    "    ax=ax[0]\n",
    ")\n",
    "\n",
    "ax[0].set(title=\"Posterior Predictive Check (train)\", xlim=(0, 1_000))\n",
    "\n",
    "# plot test posterior\n",
    "# -----------\n",
    "az.plot_ppc(\n",
    "    data=posterior_predictive_oos_regression_test, \n",
    "    kind=\"cumulative\", \n",
    "    observed_rug=True, \n",
    "    ax=ax[1]\n",
    ")\n",
    "\n",
    "# plot settings\n",
    "ax[1].set(title=\"Posterior Predictive Check (test)\", \n",
    "          xlim=(0, 1_000));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27605a4",
   "metadata": {},
   "source": [
    "Yay! The results look quite reasonable 🙂!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffaeaa",
   "metadata": {},
   "source": [
    "### Acknowledgement\n",
    "The lab was adapted from the following official documentation in PyMC and acknowledge their work in building out  the library and this helpful tutorial, which can be [viewed here](https://www.pymc.io/projects/examples/en/latest/case_studies/BART_introduction.html). Please explore the official documentation if you want to learn more. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
