{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science]\n",
    "## 4-1 Clustering and PCA - Student Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering** is an unsuperivsed ML method used to group data points based on their features alone, and no observed grouping labels as in supervised classification. Thus most clustering alorithms seeks to group points by their distance in a high dimensional space generated by provided features.\n",
    "\n",
    "Below is a plot showing the results of the clustering algorithms in `Scikit-Learn` for several different toy datasets.\n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environment\n",
    "Remember to always activate your virtual environment first before you install packages or run a notebook! This helps to prevent conflicts between dependencies across different projects and ensures that you are using the correct versions of packages. You must have created anaconda virtual enviornment in the `Anaconda Installation` lab. If you have not or want to create a new virtual environment, follow the instruction in the `Anaconda Installation` lab. \n",
    "\n",
    "<br>\n",
    "\n",
    "If you have already created a virtual enviornment, you can run the following command to activate it: \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate <virtual_env_name>`\n",
    "\n",
    "<br>\n",
    "\n",
    "For example, if your virtual environment was named as CSS, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate CSS`\n",
    "\n",
    "<br>\n",
    "\n",
    "To deactivate your virtual environment after you are done working with the lab, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda deactivate`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) K-means clustering  \n",
    "\n",
    "In this section we will cover k-means clustering using `scikit-learn`. The scikit-learn documentation for clustering is found [here](http://scikit-learn.org/stable/modules/clustering.html).\n",
    "\n",
    "First we'll import `KMeans` and `numpy` so that we can make our arrays. The `%matplotlib inline` will make our plots show up within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries \n",
    "# ----------\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# settings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off with a few points arranged in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of points\n",
    "X = np.array([[0,1], [1,2], [1, 0], [-1, -3],\n",
    "             [15, 21], [18, 30], [20, 20], [22, 19],\n",
    "             [45, 50], [42, 48], [60, 40], [50, 50]])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot them we can see that they appear to be arranged roughly in three groups. *Note: the asterisk is used for \"unpacking\" the two lists in `X.T` into the function call of `plt.scatter`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "plt.scatter(*X.T)\n",
    "plt.title('random points')\n",
    "plt.xlabel('feature0')\n",
    "plt.ylabel('feature1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our clusters, all we have to do is specify how many we want, and then fit the model to the data. We'll choose 3. We can also specify the maximum number of iterations of the k-means algorithm, which you may want to do with a much larger dataset.\n",
    "\n",
    "First thing's first: **set a random seed!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the model. We'll use the [`KMeans()`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) methods from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify kmeans algorithm \n",
    "# ----------\n",
    "kmeans = KMeans(n_clusters=3,\n",
    "                n_init=10,\n",
    "                max_iter=300 #default\n",
    "                ).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the centers of the clusters through the `cluster_centers_` attribute. To get the labels (i.e. the corresponding cluster) we use `labels_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the centers \n",
    "print(\"Centers\")\n",
    "print(kmeans.cluster_centers_)\n",
    "print()\n",
    "\n",
    "# print labels\n",
    "print(\"Labels\")\n",
    "print(kmeans.labels_)\n",
    "print()\n",
    "\n",
    "# print coordinates and labels \n",
    "for point, label in zip(X, kmeans.labels_):\n",
    "    print(\"Coordinates:\", point, \"Label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's also plot out cluster centers along with the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the centers along with the points by cluster\n",
    "# ----------\n",
    "\n",
    "# figure settings\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# plot\n",
    "ax1.scatter(*X[kmeans.labels_==0,:].T, s=50, c='r', label='Cluster 0')\n",
    "ax1.scatter(*X[kmeans.labels_==1,:].T, s=50, c='b', label='Cluster 1')\n",
    "ax1.scatter(*X[kmeans.labels_==2,:].T, s=50, c='g', label='Cluster 2')\n",
    "ax1.scatter(*kmeans.cluster_centers_.T, s=50, marker='+', c='black', label='cluster centers')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('feature0')\n",
    "plt.ylabel('feature1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see to which cluster a new point would belong, we simply use the `predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict to see which cluster a new point would belong\n",
    "# ----------\n",
    "\n",
    "# create new points\n",
    "new_points = np.asarray([[0, 4],\n",
    "                        [19, 25],\n",
    "                        [40, 50]])\n",
    "\n",
    "# print predictions\n",
    "print(\"Predictions:\")\n",
    "print()\n",
    "\n",
    "print(\"0, 4\")\n",
    "print(\"Cluster:\", kmeans.predict([[0, 4]]))\n",
    "print()\n",
    "\n",
    "print(\"19, 25\")\n",
    "print(\"Cluster:\", kmeans.predict([[19, 25]]))\n",
    "print()\n",
    "\n",
    "print(\"40, 50\")\n",
    "print(\"Cluster:\", kmeans.predict([[40, 50]]))\n",
    "\n",
    "# figure settings\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# plot new points\n",
    "ax1.scatter(*X[kmeans.labels_==0,:].T, s=50, c='r', label='Cluster 0')\n",
    "ax1.scatter(*X[kmeans.labels_==1,:].T, s=50, c='b', label='Cluster 1')\n",
    "ax1.scatter(*X[kmeans.labels_==2,:].T, s=50, c='g', label='Cluster 2')\n",
    "ax1.scatter(*kmeans.cluster_centers_.T, s=50, c='black', marker='+', label='cluster centers')\n",
    "ax1.scatter(*new_points.T, s=50, c='cyan', label='new points')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('feature0')\n",
    "plt.ylabel('feature1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Agglomerative clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll show an example of agglomerative clustering, which is a type of hierarchical clustering. The documentation is [here](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) in case you want to know more about the parameters. We'll use some of scikitlearn's toy datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "# ----------\n",
    "from sklearn import datasets\n",
    "\n",
    "# set samples \n",
    "n_samples = 1500\n",
    "\n",
    "# create synthetic datasets: moons and blob dataset\n",
    "# ----------\n",
    "\n",
    "# moons\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, \n",
    "                                  noise=.05)[0]\n",
    "\n",
    "# blobs\n",
    "blobs, blob_truth = datasets.make_blobs(n_samples=n_samples, \n",
    "                                        random_state=0)\n",
    "\n",
    "# plot each dataset\n",
    "plt.scatter(*noisy_moons.T)\n",
    "plt.ylabel('noisy moons')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(*blobs.T)\n",
    "plt.ylabel('blobs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use two clusters this time, and use ward linkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "# ----------\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# run algorithm\n",
    "# ----------\n",
    "ward = AgglomerativeClustering(n_clusters=3,\n",
    "                               linkage='ward', #linkage can be ward (default), complete, or average\n",
    "                               metric='euclidean') #affinity must be euclidean if linkage=metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit the clustering model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit to data to moons\n",
    "# ----------\n",
    "ward.fit(noisy_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll sort the points by label and then plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "# ----------\n",
    "\n",
    "# sort by labels\n",
    "zero = np.array([point for label, point in zip(ward.labels_, noisy_moons) if label == 0])\n",
    "one = np.array([point for label, point in zip(ward.labels_, noisy_moons) if label == 1])\n",
    "two = np.array([point for label, point in zip(ward.labels_, noisy_moons) if label == 2])\n",
    "\n",
    "# plot \n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(*zero.T, s=50, c='b', label='zero')\n",
    "ax1.scatter(*one.T, s=50, c='r', label='one')\n",
    "ax1.scatter(*two.T, s=50, c='g', label ='two')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do the same with the blobs dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit to blob data \n",
    "# ----------\n",
    "ward.fit(blobs)\n",
    "\n",
    "\n",
    "# plot \n",
    "# ----------\n",
    "\n",
    "# sort by labels\n",
    "zero = np.array([point for label, point in zip(ward.labels_, blobs) if label == 0])\n",
    "one = np.array([point for label, point in zip(ward.labels_, blobs) if label == 1])\n",
    "two = np.array([point for label, point in zip(ward.labels_, blobs) if label == 2])\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(*zero.T, s=50, c='b', label='zero')\n",
    "ax1.scatter(*one.T, s=50, c='r', label='one')\n",
    "ax1.scatter(*two.T, s=50, c='g', label='two')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: DBSCAN \n",
    "\n",
    "\n",
    "**It looks like our agglomerative clustering model did not cluster the noisy moons dataset how we might have wanted.** \n",
    "\n",
    "For the challenge, use [`DBSCAN`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) to cluster noisy moons. Then plot the results and see what it looks like. Try an `eps` value of .2. This sets the maximum distance between two samples for them to be considered in the same neighborhood.\n",
    "\n",
    "#### Moon data\n",
    " \n",
    "Let's see if we can better capture the clear pattern in the data using `DBSCAN`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "# ----------\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# define model object\n",
    "# ----------\n",
    "dbscan = ...\n",
    "\n",
    "# fit model to data \n",
    "# ----------\n",
    "...;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fitted labels for each data point \n",
    "labels = ...\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify number of labels and their unique score\n",
    "print(len(set(labels)))\n",
    "print(np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there any outliers not included in either cluster. They will have a value of `-1`. Additionally, the count will be not match what we found above if there are outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inferred clusters\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)  # outliers will have different values\n",
    "n_clusters_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results to see if it better captures the patterns we see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into those for each cluster \n",
    "# ----------\n",
    "zero = np.array([point for label, point in zip(..., noisy_moons) if label == 0])\n",
    "one = np.array([point for label, point in zip(..., noisy_moons) if label == 1])\n",
    "\n",
    "# plot \n",
    "# ----------\n",
    "\n",
    "# figure settings\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# plot the data with cluster assignment as the color \n",
    "ax1.scatter(*zero.T, s=50, c='b', label='zero')\n",
    "ax1.scatter(*one.T, s=50, c='r', label='one')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Blob data\n",
    "\n",
    "Now let's fit another DBSCAN model to the blobs data and see what patterns it identifies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model object\n",
    "# ----------\n",
    "dbscan = ...\n",
    "\n",
    "# fit model to data \n",
    "# ----------\n",
    "dbscan..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fitted labels for each data point \n",
    "labels = ...\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, see if there are outliers not included in any cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inferred clusters\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_clusters_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's plot the points in the blobs dataset, coloring them by their cluster id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "# ----------\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(blobs[:, ... ],blobs[:, ...], s=50, c=labels, label='zero')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) is an unsupervised machine learning technique. At a basic level, it summarizes information in many features by collapsing them into fewer features. PCA can be used for both exploratory data analysis and dimensionality reduction. For this exercise, we are going to use the [breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) from sklearn. First, let's load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "# ----------\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# process data\n",
    "# ----------\n",
    "breast = load_breast_cancer()\n",
    "\n",
    "# create an array of features \n",
    "breast_data = breast.data\n",
    "\n",
    "# create array of target data\n",
    "breast_labels = breast.target\n",
    "\n",
    "# reshape target array\n",
    "labels = np.reshape(breast_labels,(569,1))\n",
    "\n",
    "# concatenate features and labels\n",
    "final_breast_data = np.concatenate([breast_data,labels],axis=1)\n",
    "\n",
    "# get feature names\n",
    "features = breast.feature_names\n",
    "features_labels = np.append(features,'label')\n",
    "\n",
    "# coerce to dataframe and add column names\n",
    "breast_dataset = pd.DataFrame(final_breast_data)\n",
    "breast_dataset.columns = features_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out first 5 rows\n",
    "breast_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the \"label\" column that will be our target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view label column\n",
    "breast_dataset['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recode 0 to \"benign\" and 1 to \"malignant\" to make these more clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode labels\n",
    "breast_dataset['label'].replace(0, 'Benign', inplace = True)\n",
    "breast_dataset['label'].replace(1, 'Malignant', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with supervised methods, scaling our data in advance is usually a good idea. Apply the sklearn [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to the features in our dataframe and save the result as an array called \"X\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library \n",
    "# ----------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale data\n",
    "# ----------\n",
    "X = breast_dataset.loc[:, features].values\n",
    "X = StandardScaler().fit_transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply our PCA! Use the [`PCA()`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) method from sklearn to perform a PCA on the breast cancer features and summarize them with two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library \n",
    "# ----------\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# apply PCA\n",
    "# ----------\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents_breast = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into a dataframe\n",
    "# ----------\n",
    "pca_df = pd.DataFrame(data = principalComponents_breast, \n",
    "                      columns = ['principal component 1', \n",
    "                                 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view factor loadings as dataframe\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# ----------\n",
    "\n",
    "# figure settings\n",
    "plt.figure()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Principal Component - 1',fontsize=20)\n",
    "plt.ylabel('Principal Component - 2',fontsize=20)\n",
    "plt.title(\"Principal Component Analysis of Breast Cancer Dataset\",fontsize=20)\n",
    "targets = ['Benign', 'Malignant']\n",
    "colors = ['r', 'g']\n",
    "\n",
    "# loop over targets and colors to create scatterplot\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = breast_dataset['label'] == target\n",
    "    plt.scatter(pca_df.loc[indicesToKeep, 'principal component 1']\n",
    "               , pca_df.loc[indicesToKeep, 'principal component 2'], c = color, s = 50)\n",
    "\n",
    "# show figure\n",
    "plt.legend(targets,prop={'size': 15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a scree plot to help determine the number of components to keep\n",
    "\n",
    "To keep things simple in this lab, we limit the number of components above to 2. But if you wanted to know how many components to include, you might use a scree plot. Instead of specifying `n=2` above, you will not specify anything ,which will keep all components. You can then use the following code to create a scree plot using the `explained_variance_ratio_` attribute from the returned PCA object.\n",
    "\n",
    "**QUESTION:** How many components might you keep just based on the scree plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create a scree plot to evaluate the number of components to keep\n",
    "# --------------------------------------------------\n",
    "\n",
    "# fit PCA\n",
    "# ----------\n",
    "pca_all = PCA() # notice we do not specify the number of components if we want to keep them all\n",
    "principalComponents_breast = pca_all.fit_transform(X)\n",
    "\n",
    "\n",
    "# create scree plot from PCA fit\n",
    "# ----------\n",
    "PC_values = np.arange(pca_all.n_components_) + 1\n",
    "plt.plot(PC_values, pca_all.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare models using just the two components from the PCA with one that does not use any components to see how PCA might be helpful. \n",
    "\n",
    "First, train a logistic regression that predicts the label using all of the features. Then train a second logistic regression model that uses only the principal components as features. Let's see how they compare using confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "# ----------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All features\n",
    "\n",
    "First, use all the features to fit a logisitic model. Fill in the code below to fit a model that uses all the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# using all features\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# create target dataset\n",
    "# ----------\n",
    "y = breast_dataset...\n",
    "\n",
    "# create features dataset - give it a different name so we can reuse it in the second step for PCA\n",
    "# ----------\n",
    "X_original = breast_dataset...\n",
    "\n",
    "# split data\n",
    "# ----------\n",
    "X_train, X_test, y_train, y_test = train_test_split(...,              # specify features dataset - use \"original\" here\n",
    "                                                    ...,              # specify labels dataset\n",
    "                                                    train_size = ..., # specify training dataset size\n",
    "                                                    test_size=...,    # specify testing dataset size\n",
    "                                                    random_state=10)  # set random seed\n",
    "\n",
    "\n",
    "# initalize a logistic regresssion model\n",
    "# ----------\n",
    "logit_reg = LogisticRegression(max_iter= 5000) \n",
    "\n",
    "\n",
    "# fit the model to the training data\n",
    "# ----------\n",
    "logit_model = logit_reg.fit(..., \n",
    "                            ...)\n",
    "\n",
    "# predict on the test set\n",
    "# ----------\n",
    "y_pred = logit_model.predict(...)\n",
    "\n",
    "\n",
    "# create a confusion matrix\n",
    "# ----------\n",
    "cf_matrix = confusion_matrix(..., \n",
    "                             ..., \n",
    "                             ...)      # make sure to normalize\n",
    "\n",
    "\n",
    "# create a pandas dataset from the confusion matrix\n",
    "# ----------\n",
    "df_cm = pd.DataFrame(cf_matrix, \n",
    "                     range(2),\n",
    "                     range(2))\n",
    "\n",
    "\n",
    "# plot the confusion matrix\n",
    "# ----------\n",
    "df_cm = df_cm.rename(index=str, columns={0: \"Benign\", 1: \"Malignant\"})\n",
    "df_cm.index = [\"Benign\", \"Malignant\"]\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "# plot labels\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA components\n",
    "\n",
    "Now, see what a model looks like just using the two components from the PCA. Fill in the code below and compare to results from the model that uses all the features above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# using PCA Features\n",
    "# --------------------------------------------------------\n",
    "\n",
    "\n",
    "# create target dataset\n",
    "# ----------\n",
    "y = breast_dataset...\n",
    "\n",
    "\n",
    "# create features dataset \n",
    "# ----------\n",
    "X = pca_df\n",
    "\n",
    "\n",
    "\n",
    "# split data\n",
    "# ----------\n",
    "X_train, X_test, y_train, y_test = train_test_split(..., \n",
    "                                                    ...,\n",
    "                                                    train_size = ...., \n",
    "                                                    test_size=.... \n",
    "                                                    random_state=10)\n",
    "\n",
    "\n",
    "# initalize a logistic regresssion model\n",
    "# ----------\n",
    "logit_reg = LogisticRegression(max_iter= 5000) \n",
    "\n",
    "\n",
    "\n",
    "# fit the model\n",
    "# ----------\n",
    "logit_model = logit_reg.fit(..., \n",
    "                            ...)\n",
    "\n",
    "\n",
    "# predict on test set\n",
    "# ----------\n",
    "y_pred = logit_model.predict(...)\n",
    "\n",
    "\n",
    "# create a confusion matrix\n",
    "# ----------\n",
    "cf_matrix = confusion_matrix(..., \n",
    "                             ..., \n",
    "                             ...)              # make sure to normalize\n",
    "\n",
    "# create a pandas dataframe from the confusion matrix\n",
    "# ----------\n",
    "df_cm = pd.DataFrame(cf_matrix, \n",
    "                     range(2),\n",
    "                     range(2))\n",
    "\n",
    "\n",
    "# plot the confusion matrix\n",
    "# ----------\n",
    "df_cm = df_cm.rename(index=str, \n",
    "                     columns={0: \"Benign\", 1: \"Malignant\"})\n",
    "df_cm.index = [\"Benign\", \"Malignant\"]\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "# specify labels\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: How did the logistic regression trained on just the PCA features compare to the original?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Authored by Aniket Kesari. Materials borrowed from D-Lab's [Python Machine Learning Workshop](https://github.com/dlab-berkeley/python-machine-learning/blob/master/3_clustering.ipynb), and [datacamp](https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python). Slight additions by Kasey Zapatka in 2023."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
