{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science]\n",
    "## 5-3 Text Feature Engineering and Classification - Student Version\n",
    "\n",
    "In this lab we will use the techniques we covered so far to engineer text features and train a classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environment\n",
    "Remember to always activate your virtual environment first before you install packages or run a notebook! This helps to prevent conflicts between dependencies across different projects and ensures that you are using the correct versions of packages. You must have created anaconda virtual enviornment in the `Anaconda Installation` lab. If you have not or want to create a new virtual environment, follow the instruction in the `Anaconda Installation` lab. \n",
    "\n",
    "<br>\n",
    "\n",
    "If you have already created a virtual enviornment, you can run the following command to activate it: \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate <virtual_env_name>`\n",
    "\n",
    "<br>\n",
    "\n",
    "For example, if your virtual environment was named as CSS, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate CSS`\n",
    "\n",
    "<br>\n",
    "\n",
    "To deactivate your virtual environment after you are done working with the lab, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda deactivate`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "# ----------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# settings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../../images/cfpb_logo.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll once again use the Consumer Financial Protection Bureau's [Consumer Complaint Database](https://www.consumerfinance.gov/data-research/consumer-complaints/). Picking up from where we left off last time, we'll focus on predicting whether a consumer complaint narrative is talking about a \"checkings or savings account\" issue or a \"student loan\" issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# ----------\n",
    "# load data\n",
    "cfpb = pd.read_csv(\"../../data/CFPB 2020 Complaints.csv\")\n",
    "\n",
    "# drop missing on \"Consumer complaint narrative\" feature and reset the index bc we've dropped\n",
    "cfpb = cfpb.dropna(subset = ['Consumer complaint narrative'])\n",
    "\n",
    "# filtering to keep only savings and student loan products\n",
    "cfpb = cfpb[(cfpb['Product']=='Checking or savings account') | (cfpb['Product'] == 'Student loan')]\n",
    "\n",
    "# filtering to keep the first 1000 rows\n",
    "cfpb = cfpb[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do any feature engineering or classification, we should first preprocess our text. Let's start by adding custom stop words and defining our custom `rem_punc_stop()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a few words analysis-specific words to our Stop Words\n",
    "# ----------\n",
    "STOP_WORDS = STOP_WORDS.union({\"XX\", \"XXXX\",\"XXXXXXXX\"})\n",
    "\n",
    "# what are some ways we might check to see if these stop words were added\n",
    "# ----------\n",
    "\n",
    "# option 1\n",
    "... \n",
    "\n",
    "# option 2\n",
    "... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHALLENGE:** Label what each line is doing. See the \"Solutions\" notebook for answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create preprocessing function - like we have in the past few labs \n",
    "# ----------\n",
    "def rem_punc_stop(text):\n",
    "    \n",
    "    # ... \n",
    "    punc = set(punctuation)\n",
    "    \n",
    "    # ... \n",
    "    punc_free = \"\".join([ch for ch in text if ch not in punc])\n",
    "    \n",
    "    # ... \n",
    "    doc = nlp(punc_free)\n",
    "    \n",
    "    # ... \n",
    "    spacy_words = [token.text for token in doc]\n",
    "    \n",
    "    # ... \n",
    "    no_punc = [word for word in spacy_words if word not in STOP_WORDS]\n",
    "    \n",
    "    # ... \n",
    "    return no_punc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go ahead and apply our function to the consumer complaint narratives. Notice how the `rem_punc_stop()` function returns a list, but we can collapse our tokens back into strings with the `join()` string method.\n",
    "\n",
    "Note that this will take a few moments to run, so be patient. \n",
    "\n",
    "\n",
    "**CHALLENGE:** This code has been altered from the solutions script to produce an error. What is the error? and how woudl you fix it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply our preprocessing function to the consumer complaint column in our original dataframe\n",
    "# ----------\n",
    "\n",
    "# remove punctuation and stop words\n",
    "cfpb['tokens'] = cfpb['Consumer complaint narrative'].map(lambda x: rem_punc_stop(x))\n",
    "\n",
    "# collapse tokens in a string using join method since output of last step is a list\n",
    "cfpb['tokens'] = cfpb['tokens'].map(lambda text: ''.join(text))\n",
    "\n",
    "# view\n",
    "cfpb['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already explored several exploratory data analysis techniques. There are many different ways to explore text data that we haven't covered, but let's take a look at one last basic tool: visualizing n-grams.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Bag-of-Words CounterVectorizer\n",
    "# ----------\n",
    "# notice the ngram_range argument\n",
    "countvec = CountVectorizer(stop_words=list(STOP_WORDS), # specify a list of stop words to extract \n",
    "                           ngram_range=(...,...))           # specify bi- and trigrams \n",
    "\n",
    "# fit and transform on tokens\n",
    "ngrams = countvec.fit_transform(cfpb['tokens'])         \n",
    "\n",
    "# create a dataframe out of ngram sparse matrix\n",
    "dictionary_dataframe = pd.DataFrame(ngrams.todense(), # why do you need to specify tondense() here?\n",
    "                                    columns = countvec.get_feature_names_out()) # get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of n-grams and their frequency \n",
    "# ----------\n",
    "\n",
    "# create a datamframe by summing over the dictionary_dataframe from above and resetting the index\n",
    "df_ngram = pd.DataFrame(dictionary_dataframe.sum().reset_index()).rename(columns={'index': 'ngrams', 0:'freq'})\n",
    "\n",
    "#  sort by frequency \n",
    "df_ngram = df_ngram.sort_values(by = ['freq'], \n",
    "                                ascending = False).reset_index(drop = True)\n",
    "\n",
    "# view the first few lines\n",
    "df_ngram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bar plot visualization\n",
    "# ----------\n",
    "sns.barplot(x=\"...\",    # what goes on the x-axis here?\n",
    "            y = '...',  # what goes on the y-axis here?\n",
    "            data=df_ngram[0:25])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHALLENGE:** Adjust the code above to visualize the most popular unigrams and 4-grams. What is the tradeoff involved with increasing n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust to count unigrams \n",
    "# ----------\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust to count 4-grams\n",
    "# ----------\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time, we saw some techniques for exploring the text of our data. Specifically, we saw how to find the length of our text and word counts. How can we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the character count for each document \n",
    "# --------\n",
    "# apply length function to tokens column to count each character - what \n",
    "cfpb['complaint_len'] = cfpb['tokens']... \n",
    "\n",
    "# view the new feature we just created\n",
    "cfpb[['tokens', 'complaint_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the word count for each document \n",
    "# --------\n",
    "# apply length function to tokens column to count words -- you will need to split the text to count words\n",
    "cfpb['word_count'] = cfpb['tokens']...\n",
    "\n",
    "# view\n",
    "cfpb[['tokens', 'complaint_len', 'word_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also covered subjectivity and sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create polarity and subjectivity features\n",
    "# --------\n",
    "cfpb['polarity'] = cfpb['tokens'].map(lambda text: TextBlob(...).sentiment. ...)\n",
    "cfpb['subjectivity'] = cfpb['tokens'].map(lambda text: TextBlob(...).sentiment. ...)\n",
    "\n",
    "# view\n",
    "cfpb[['tokens', 'complaint_len', 'word_count', 'polarity', 'subjectivity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's take the top 25 n-grams we found earlier and turn them into their own dataframe. We'll return to these later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dictionary\n",
    "# --------\n",
    "\n",
    "# create a count of words after dropping stop words\n",
    "countvec = CountVectorizer(stop_words=list(STOP_WORDS),  # use our list of stop words\n",
    "                           ngram_range=(2,3))            # specify bi- and tri-grams\n",
    "\n",
    "# apply countvec function to our tokenized data\n",
    "ngrams = countvec.fit_transform(cfpb['tokens'])\n",
    "\n",
    "# create dictionary - but note \n",
    "dictionary_dataframe = pd.DataFrame(ngrams.todense(), \n",
    "                                    columns = countvec.get_feature_names_out())\n",
    "\n",
    "\n",
    "# create ngram frequency dataframe by summing over dictionary dataframe\n",
    "df_ngram = pd.DataFrame(dictionary_dataframe.sum().reset_index()).rename(columns={'index': 'ngrams', 0:'freq'})\n",
    "df_ngram = df_ngram.sort_values(by = ['freq'], \n",
    "                                ascending = False).reset_index(drop = True)\n",
    "\n",
    "# view just a subset\n",
    "top_25_ngrams = dictionary_dataframe.loc[:,df_ngram[0:25]['ngrams']]\n",
    "top_25_ngrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code we went over from the last lab, make a dataframe with 5 LDA generated topics. Then, create a topic model using [Non-Negative Matrix Factorization](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) and print out the words associated with the first 5 topics. NMF is another algorithm that is frequently used for topic modeling. \n",
    "\n",
    "**QUESTION:** Do you get similar topics as with your LDA topics? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's define a function to print the top words that we'll use in our model\n",
    "# --------\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #{}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Latent Dirichlet Allocation (LDA)\n",
    "# ---------------------------------------\n",
    "\n",
    "# pre-processing\n",
    "# --------\n",
    "# create a new data object called X\n",
    "X = cfpb['tokens']\n",
    "\n",
    "# initialize tf-idf function and set parameters\n",
    "tf = TfidfVectorizer(...,  # specify our function for remove punc and stop words\n",
    "                     ...)       # what can we add here to remove the Warning? Is this necessary?\n",
    "\n",
    "\n",
    "# apply tf-idf vectorizer to our data (X)\n",
    "tfidf_matrix =  tf.fit_transform()  # specify correct data ehre\n",
    "\n",
    "# modify the output to be a dense matrix\n",
    "dense_matrix = tfidf_matrix... # how do we densify this matrix?\n",
    "\n",
    "# intitialize LDA model and \n",
    "# --------\n",
    "# initialize LDA and set model parameters\n",
    "lda = LatentDirichletAllocation(...,            # specify 5 components\n",
    "                                ...,            # specify 20 iterations \n",
    "                                random_state=0) # set seed of 0 for  reproducibility\n",
    "\n",
    "# fit LDA model to our dense matrix\n",
    "lda = lda.fit(np.asarray(dense_matrix))\n",
    "\n",
    "# post-processing\n",
    "# --------\n",
    "# get feature names from our tf-idf vector\n",
    "tf_feature_names = tf.get_feature_names_out()\n",
    "\n",
    "# print top words \n",
    "print_top_words(...,  # specify LDA model\n",
    "                ...,  # specify feature names vector\n",
    "                ...)  # specify that we want to see 20 words here\n",
    " \n",
    "\n",
    "# now transform our data using the lda model and create a dataframe\n",
    "topic_dist = lda.transform(tfidf_matrix)\n",
    "topic_dist_df = pd.DataFrame(topic_dist).reset_index(drop = True)\n",
    "\n",
    "# view the corresponding tf-idf dataframe with tf-idf values\n",
    "topic_dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Non-Negative Matrix Factorization (NMF)\n",
    "# ---------------------------------------\n",
    "\n",
    "# pre-processing\n",
    "# --------\n",
    "# create a new data object called X\n",
    "X = cfpb['tokens']\n",
    "\n",
    "# initialize tf-idf function and set parameters\n",
    "tf = TfidfVectorizer(tokenizer = rem_punc_stop,  # specify our function for remove punc and stop words\n",
    "                     token_pattern = None)       # specify \"None\" to remove warning. Is this necessary?\n",
    "\n",
    "# apply tf-idf vectorizer to our data (X)\n",
    "tfidf_matrix =  tf.fit_transform(X)\n",
    "\n",
    "# modify the output to be a dense matrix\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "\n",
    "# intitialize LDA model and \n",
    "# --------\n",
    "# initialize LDA and set model parameters\n",
    "nmf = NMF(...,             # specify the number of components to 5\n",
    "          ...,             # specify a random initalization method\n",
    "          random_state=1)  # set a seed for reproducibility\n",
    "\n",
    "# fit NMF model to our dense matrix\n",
    "nmf = nmf.fit(np.asarray(dense_matrix))\n",
    "\n",
    "# post-processing\n",
    "# --------\n",
    "# get feature names from our tf-idf vector\n",
    "tf_feature_names = tf.get_feature_names_out()\n",
    "\n",
    "# print top words \n",
    "print_top_words(nmf,               # specify model\n",
    "                tf_feature_names,  # specify feature names vector\n",
    "                20)                # specify how many words we want to see\n",
    "\n",
    "\n",
    "# now transform our data using the lda model and create a dataframe\n",
    "nmf_topic_dist = nmf.transform(tfidf_matrix)\n",
    "nmf_topic_dist_df = pd.DataFrame(nmf_topic_dist).reset_index(drop = True)\n",
    "\n",
    "# view the corresponding tf-idf dataframe with tf-idf values\n",
    "nmf_topic_dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to move to classification! We are going to examine how different featurization techniques compare. Create a list with the following:\n",
    " * Text Engineered Features\n",
    " * Text Engineered Features + Topic Model\n",
    " * Non-Text Features only\n",
    " * Non-Text Features + Text Engineered Features\n",
    " * Tf-idf\n",
    " * Non-Text Features + tf-idf\n",
    " * Top 25 n-gram\n",
    " * Non-Text Features + Top 25 n-gram\n",
    " \n",
    "You'll need to use pandas to create and .`join()` these different dataframes together. Also be sure to use `reset_index()` as necessary. Once you've created each of these dataframes (or arrays!) you should loop through all of them, train a supervised learning algorithm (like logistic regression or a decision tree classifier), and plot confusion matrices. Once you do this, think about which featurization technique worked the best, and whether combining text and non-text features was helpful. For now, don't worry about hyperparameter tuning or feature selection, though you would do these in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first take a look at the column names to get a sense of what we have\n",
    "# --------\n",
    "cfpb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# pre-processing for a classification model\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "# Engineered Text Features\n",
    "# --------\n",
    "# select engineered text features   # specify 4 engineered text features from above\n",
    "engineered_features = cfpb[['...', \n",
    "                            '...', \n",
    "                            '...', \n",
    "                            '...']].reset_index(drop = True)\n",
    "\n",
    "# Topic Model + Engineered Features\n",
    "# --------\n",
    "# combine results from our topic model + engineered features\n",
    "engineered_features_with_topics = topic_dist_df.join(....reset_index(drop = True)) # engineered features\n",
    "\n",
    "\n",
    "# Non-text features\n",
    "# --------\n",
    "# select non-text features\n",
    "non_text_features = cfpb[['...',  # specify 7 non-text features\n",
    "                          '...',\n",
    "                          '...', \n",
    "                          '...',\n",
    "                          '...', \n",
    "                          '...',\n",
    "                          '...']]\n",
    "\n",
    "# get dummies of non-text features\n",
    "non_text_features_dummies = pd....reset_index(drop = True) # get dummies of non_text_features\n",
    "\n",
    "\n",
    "\n",
    "# Non-text features + engineered features\n",
    "# --------\n",
    "# combine non-text features + engineered features with topics\n",
    "non_text_engineered_features = ....reset_index(drop = True).join(...)\n",
    "\n",
    "\n",
    "# Non-text features + tfidf\n",
    "# --------\n",
    "\n",
    "# create dataframe of tf-idf \n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.todense(),                # specify matrix\n",
    "                        columns = ... . ...())  # get feature names from tf object\n",
    "\n",
    "# combine non-text features with tf-idf  \n",
    "non_text_plus_tfidf = non_text_features_dummies.reset_index(drop = True).join(...) # specify tf-idf\n",
    "\n",
    "\n",
    "# Non-Text Features + Top 25 n-gram\n",
    "# --------\n",
    "# combine non-text features and Top 25 ngrams (from above)\n",
    "non_text_with_ngrams = non_text_features_dummies.reset_index(drop = True).join(...) # specify top_25_ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more pre-processing \n",
    "# --------\n",
    "\n",
    "# DESCRIBE WHAT IS HAPPENING AT EACH STEP WITH ANNOTATIONS\n",
    "\n",
    "# ...\n",
    "dataframes = [engineered_features,   \n",
    "              engineered_features_with_topics,\n",
    "              non_text_features_dummies,\n",
    "              non_text_engineered_features,\n",
    "              tfidf_df, \n",
    "              non_text_plus_tfidf,\n",
    "              top_25_ngrams,\n",
    "              non_text_with_ngrams]\n",
    "\n",
    "\n",
    "# ...\n",
    "featurization_technique = ['Engineered Text Features',\n",
    "                           'Engineered Features + Topic Model',\n",
    "                           'Non-Text Features',\n",
    "                           'Non-Text + Engineered Text Features',\n",
    "                           'Tf-idf Features',\n",
    "                           'Non-Text + Tf-idf Features',\n",
    "                           'Top 25 N-Gram Features',\n",
    "                           'Non-Text + Top 25 N-Gram Features']\n",
    "\n",
    "# model initalization and fit\n",
    "# --------\n",
    "\n",
    "# ...\n",
    "lb_style = LabelBinarizer()\n",
    "\n",
    "# ...\n",
    "y = cfpb['Product_binary'] = lb_style.fit_transform(cfpb[\"Product\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each dataframe, apply a logit classification model, and create a confusion matrix\n",
    "# --------\n",
    "for dataframe, featurization in zip(dataframes, featurization_technique):\n",
    "    \n",
    "    # changing column names to strings because some are integer\n",
    "    dataframe.columns = dataframe.columns.astype(str) \n",
    "    \n",
    "    # split data\n",
    "    # ----------\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataframe,         # specify features\n",
    "                                                        y,                 # specify labels\n",
    "                                                        ...,               # specify train split of 80%\n",
    "                                                        ...,               # specify test split of 20%\n",
    "                                                        random_state = 10) # set seed for reproducibility\n",
    "    # apply Logit model \n",
    "    # ----------\n",
    "    # initalize Logit model \n",
    "    logit_reg = ...\n",
    "\n",
    "    # fit the model to our data\n",
    "    logit_model = logit_reg.fit(...,          # specify feature data\n",
    "                                ... .ravel()) # specify label data --- don't forget to ravel the training dataset\n",
    "\n",
    "    # predict on test set\n",
    "    y_pred = logit_model.predict(...)      # predict on test data\n",
    "    \n",
    "    # create a confusion matrix\n",
    "    # ----------\n",
    "    cf_matrix = confusion_matrix(...,             # specify test\n",
    "                                 ...,             # specify predicted\n",
    "                                 ...)             # normalize\n",
    "\n",
    "    # create a pandas dataframe from the confusion matrix for visualization \n",
    "    df_cm = pd.DataFrame(...,                # specify cf_matrix from which to create dataframe \n",
    "                         range(2), range(2)) # set 2 by 2 \n",
    "\n",
    "    # set confusion matrix indices for clarity of visualization\n",
    "    df_cm = df_cm.rename(index=str, \n",
    "                         columns={0: \"Checking or savings account\", \n",
    "                                  1: \"Student loan\"})\n",
    "\n",
    "    # specify confusion matrix index for clarity\n",
    "    df_cm.index = [\"Checking or savings account\", \"Student loan\"]\n",
    "    \n",
    "    # figure specifications \n",
    "    plt.figure(figsize = (10,7)) # set figure size specifications\n",
    "    sns.set(font_scale=1.4)      # set label size\n",
    "    sns.heatmap(df_cm,           # specify dataset\n",
    "                annot=True,    \n",
    "                annot_kws={\"size\": 16},\n",
    "                fmt='g')\n",
    "    \n",
    "    # figure labels\n",
    "    plt.title(featurization)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, combing text with non-text features will improve a classifier's performance. However this isn't automatic - in some cases you can actually degrade a classifier's performance by adding in more features (e.g., Non-text alone out-performed Non-text + Engineered).\n",
    "\n",
    "In this case, our engineered features were too quick to predict \"checking or savings account\" and our tf-idf alone performed just as well as tf-idf + non-text features. \n",
    "\n",
    "However, non-text features + ngrams and tf-idf tied in accuracy performance. We might prefer the former approach because it is computationally cheaper, and likely easier to explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "[Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#:~:text=Naive%20Bayes%20methods%20are%20a,value%20of%20the%20class%20variable) is a supervised learning algorithim that leverages [Bayes’ theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) that is commonly used to solve classification problems. The \"naive\" of Naive Bayes comes from its assumption of conditional independence between every feature pair during estimation. In the context of Natural Language Processing, this means it assumes the probabilty a word will be chosen is independent of other words in the corpus. While this strong assumption seems to violate what we know about language, the model has proven time and time again to be very effective. \n",
    "\n",
    "One important note, however, is that although naive Bayes is known as a decent classifier, [it is known to be a bad estimator](https://scikit-learn.org/stable/modules/naive_bayes.html#:~:text=Naive%20Bayes%20methods%20are%20a,value%20of%20the%20class%20variable.), so the probability outputs from `predict_proba` are not to be taken too seriously.\n",
    "\n",
    "Let's see how it compares to the logit models we just implemented. Note that we'll use the last dataframe (X_train) that was generated from the loop above (\"Non-Text + Top 25 N-Gram Features\"), so we should compare our results to the last confusion matrix.\n",
    "\n",
    "One last thing to note is that the library name for each model refers to the type of data used, not necessarily the outcome. So, we'll use Multinomial Naive Bayes because it works for text data not because we are modeling a multi-cateogry outcome. See this [Stack Overflow post](https://stackoverflow.com/questions/65627712/multinomialnb-or-gaussiannb-or-categoricalnb-what-to-use-here) for more on the difference between the Naive Bayes libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Multinomial Naive Bayes algorithim\n",
    "# ----------\n",
    "# load library\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# initialize the native bayes estimator\n",
    "nb = MultinomialNB(alpha = ... )   # see how performance changes with different levels of smoothing\n",
    "\n",
    "# fit it on the training data\n",
    "# ----------\n",
    "nb_model = nb.fit(..., \n",
    "                  ....ravel()) # use the .ravel() method to flatten flatten multi-dimensional to 1-dimensional array\n",
    "\n",
    "# predict on test dataset\n",
    "# ----------\n",
    "nb_pred = nb_model.predict(...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print average accuracy\n",
    "# ----------\n",
    "print(np.mean(nb_pred == y_test))\n",
    "\n",
    "# create a confusion matix\n",
    "# ----------\n",
    "nb_cf_matrix = confusion_matrix(...,  # specify test and predictions to compare\n",
    "                                ..., \n",
    "                                normalize = \"true\")\n",
    "\n",
    "# print confusion matrix scores\n",
    "print(nb_cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe for visualization\n",
    "# ----------\n",
    "nb_df_cm = pd.DataFrame(...,       # set data to convert to dataframe\n",
    "                        range(2),\n",
    "                        range(2))\n",
    "\n",
    "# visualize\n",
    "nb_df_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** How does this compare to the Logit classification model from above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix using Seaborn code\n",
    "# ----------\n",
    "# add indices and labels\n",
    "nb_df_cm = nb_df_cm.rename(index=str, columns={0: \"Checking or savings account\", \n",
    "                                               1: \"Student loan\"})\n",
    "\n",
    "nb_df_cm.index = [\"Checking or savings account\", \"Student loan\"]\n",
    "\n",
    "# specifyc figure parameters\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "\n",
    "# specify figure using seaborn library\n",
    "sns.heatmap(nb_df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')  # prevent scientific notation\n",
    "\n",
    "# figure labels\n",
    "plt.title(\"Naive Bayes Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by Aniket Kesari. Modified by Prashant Sharma (2023) and by Kasey Zapatka (2024)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
