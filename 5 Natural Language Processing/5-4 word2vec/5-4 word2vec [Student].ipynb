{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science]\n",
    "## 5-4 word2vec - Student Version\n",
    "\n",
    "In this lab we will use the techniques we introduce word embeddings via word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environment\n",
    "Remember to always activate your virtual environment first before you install packages or run a notebook! This helps to prevent conflicts between dependencies across different projects and ensures that you are using the correct versions of packages. You must have created anaconda virtual enviornment in the `Anaconda Installation` lab. If you have not or want to create a new virtual environment, follow the instruction in the `Anaconda Installation` lab. \n",
    "\n",
    "<br>\n",
    "\n",
    "If you have already created a virtual enviornment, you can run the following command to activate it: \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate <virtual_env_name>`\n",
    "\n",
    "<br>\n",
    "\n",
    "For example, if your virtual environment was named as CSS, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate CSS`\n",
    "\n",
    "<br>\n",
    "\n",
    "To deactivate your virtual environment after you are done working with the lab, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda deactivate`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and install new libraries -- WILL NEED TO UNCOMMENT AND RUN ONLY FIRST \n",
    "# ----------\n",
    "#!pip install gensim\n",
    "#!pip install tqdm\n",
    "#!pip install adjustText\n",
    "#!pip install multiprocessing - ONLY NECESSARY ON OLD VERSIONS OF PYTHON (BEFORE 2.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "# ----------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../../images/cfpb_logo.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll once again use the Consumer Financial Protection Bureau's [Consumer Complaint Database](https://www.consumerfinance.gov/data-research/consumer-complaints/). Picking up from where we left off last time, we'll focus on predicting whether a consumer complaint narrative is talking about a \"mortgage\" issue or a \"student loan\" issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# ----------\n",
    "cfpb = pd.read_csv(\"../../data/CFPB 2020 Complaints.csv\")\n",
    "cfpb = cfpb.dropna(subset = ['Consumer complaint narrative'])\n",
    "cfpb = cfpb[(cfpb['Product']=='Mortgage') | (cfpb['Product'] == 'Student loan')]\n",
    "cfpb = cfpb[:1000].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a id='context'></a>\n",
    "\n",
    "In this lab, we will be turning individual words in the data set into vectors, called \"Word Embeddings\". **Word embedding** attempt to identify semantic relationships between words by observing them in the context that the word appears. `Word2Vec` is the most prominent word embedding algorithm.\n",
    "\n",
    "Imagine that each word in a novel has its meaning determined by the ones that surround it in a limited window. For example, in Moby Dick's first sentence, “me” is paired on either side by “Call” and “Ishmael.” (`“Call me Ishmael\"`). After observing the windows around every word in the novel (or many novels), the computer will notice a pattern in which “me” falls between similar pairs of words to “her,” “him,” or “them.” Of course, the computer had gone through a similar process over the words “Call” and “Ishmael,” for which “me” is reciprocally part of their contexts.  This chaining of signifiers to one another mirrors some of humanists' most sophisticated interpretative frameworks of language.\n",
    "\n",
    "The two main flavors of `Word2Vec` are CBOW (Continuous Bag of Words) and Skip-Gram, which can be distinguished partly by their input and output during training. **Skip-Grams** take a word of interest as its input (e.g., \"me\") and tries to learn how to predict its context words (\"Call\", \"Ishmael\"). **CBOW** does the opposite, taking the context words (\"Call\", \"Ishmael\") as a single input and tries to predict the word of interest (\"me\").\n",
    "\n",
    "In general, CBOW is is faster and does well with frequent words, while Skip-Gram potentially represents rare words better.\n",
    "\n",
    "### Word2Vec Features\n",
    "\n",
    "* `vector_size`: Number of dimensions for word embedding model (*formerly* `size`) \n",
    "* `window`: Number of context words to observe in each direction\n",
    "* `min_count`: Minimum frequency for words included in model\n",
    "* `sg` (Skip-Gram): '0' indicates CBOW model; '1' indicates Skip-Gram\n",
    "* `alpha`: Learning rate (initial); prevents model from over-correcting, enables finer tuning\n",
    "* `epochs`: Number of passes through dataset (*formerly* `iterations`) \n",
    "* `batch_words`: Number of words to sample from data during each pass\n",
    "\n",
    "\n",
    "For more detailed background on Word2Vec's mechanics, I suggest this  <a href=\"https://www.tensorflow.org/text/tutorials/word2vec\">brief tutorial</a> by Google, especially the sections \"Motivation,\" \"Skip-Gram Model,\" and \"Visualizing.\" There is also this [tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py) that might be helpful as well. \n",
    "\n",
    "We will be using the default value for most of our parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's use our handy preprocessing function. Notice that this version will return a list of tokens (not a string), and we also added the `str.lower()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create preprocessing function - like we have in the past few labs\n",
    "# ----------\n",
    "\n",
    "def rem_punc_stop(text):\n",
    "    stop_words = STOP_WORDS\n",
    "    # Individually\n",
    "    # nlp.Defaults.stop_words.add(\"XX\")\n",
    "    # nlp.Defaults.stop_words.add(\"XXXX\")\n",
    "    # nlp.Defaults.stop_words.add(\"XXXXXXX\")\n",
    "    \n",
    "    # Using the bitwise |= (or) operator\n",
    "    nlp.Defaults.stop_words |= {\"xx\", \"xxxx\",\"xxxxxxxx\"}\n",
    "    \n",
    "    punc = set(punctuation)\n",
    "    \n",
    "    punc_free = \"\".join([ch for ch in text if ch not in punc])\n",
    "    \n",
    "    doc = nlp(punc_free)\n",
    "    \n",
    "    spacy_words = [token.text.lower() for token in doc]\n",
    "    \n",
    "    no_punc = [word for word in spacy_words if word not in stop_words]\n",
    "    \n",
    "    return no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply our preprocessing function to the consumer complaint column in our original dataframe\n",
    "# ----------\n",
    "cfpb['tokens'] = cfpb['Consumer complaint narrative'].map(lambda x: rem_punc_stop(x))\n",
    "cfpb['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now that we have pre-processed our text, we can use the [`gensim`](https://radimrehurek.com/gensim/models/word2vec.html) library to construct our word embeddings. We will use the Continous Bag of Words model (CBOW), which predicts target words from its neighboring context words to learn word embeddings from raw text.\n",
    "\n",
    "Read through the documentation of the Word2Vec method in gensim to understand how to implement the Word2Vec model. Then fill in the blanks so that: we use a __Continuous Bag of Words__ model to create word embeddings of __vector_size 100__ for words that appear in the `text` at least __5 or more times__. Set the learning rate to .025, epochs/number of iterations to 5, and sample 10000 words from the data during each pass.\n",
    "\n",
    "**CHALLENGE:** Go ahead and annotate each line so you know what each one is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply CBOW Word2vec model to our cfpb data\n",
    "# ----------\n",
    "model = gensim.models.Word2Vec(cfpb['tokens'],   # specify data - sentences\n",
    "                               vector_size=...,  # ...\n",
    "                               window=...,       # ...\n",
    "                               min_count=...,    # ...\n",
    "                               sg=0,             # ...\n",
    "                               alpha=...,        # ... \n",
    "                               epochs=...,       # ... \n",
    "                               seed=1,           # set random seed (same as random_state in sklearn )\n",
    "                               batch_words=...,  # ...\n",
    "                               workers = 1)      # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice with Embeddings <a id='subsection 2'></a>\n",
    "\n",
    "Now that we've trained the mode, we can return the actual high-dimensional vector by simply indexing the model with the word as the key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return embeddings for specific word \n",
    "# ----------\n",
    "print(model.wv.__getitem__(['account']))  # specify a key word here: \"account\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Check out the shape of the vectors for 'account', what do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shape\n",
    "# ----------\n",
    "model.wv.__getitem__(['account'])... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHALLENGE:** Use the following empty cells to look at what the word embeddings look like for words you think may appear in the text, for example, `bank`. Keep in mind that even if a word shows up in the text as seen above, a word vector will not be created unless it satisfies all conditions we inputted into the model above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word 1\n",
    "# ----------\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word 2\n",
    "# ----------\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word 3 \n",
    "# ----------\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're curious, the cell directly below will return a list of words that have been turned into word vectors by the model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a list of word for which we have calculations\n",
    "# ----------\n",
    "words = list(model.wv.index_to_key)\n",
    "print(words[0:100])  # print the first 100 wordsd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gensim` comes with some handy methods to analyze word relationships. `similarity` will give us a number from 0-1 based on how similar two words are. If this sounds like cosine similarity for words, you'd be right! It just takes the cosine similarity of the high dimensional vectors we input. \n",
    "\n",
    "In the following cell, find the similarity between the words `credit` and `debt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarty between credit and debt\n",
    "# ----------\n",
    "model.wv.similarity('credit', \n",
    "                    'debt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find cosine distance between two clusters of word vectors. Each cluster is measured as the mean of its words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between credit/debt and loan/mortgage\n",
    "# ----------\n",
    "model.wv.n_similarity(['credit','debt'],\n",
    "                      ['loan','mortgage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find words that don't belong with `doesnt_match`. It finds the mean vector of the words in the `list`, and identifies the furthest away. Out of the three words in the list `['credit', 'loan', 'student']`, which is the furthest vector from the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which doesn't belong? 1\n",
    "# ----------\n",
    "model.wv.doesnt_match(['credit', 'loan', 'student'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which doesn't belong? 2\n",
    "# ----------\n",
    "model.wv.doesnt_match(('pandemic', 'covid', 'bank'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most famous implementation of this vector math is semantics. What happens if we take:\n",
    "\n",
    "$$\\vec{house} - \\vec{rent} + \\vec{loan} = $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector math\n",
    "# ----------\n",
    "model.wv.most_similar(positive=['house', 'loan'], \n",
    "                      negative=['rent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a few minutes to try looking at some more vector similarity and differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more vector math\n",
    "# ----------\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis <a id='section 2'></a>\n",
    "\n",
    "Next we will explore the word embeddings of our `text` visually with PCA. We can retrieve __all__ of the vectors from a trained model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve vectors from trained model\n",
    "# ----------\n",
    "X = model.wv.__getitem__(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we do with non-text features, we want to standardize X so that all features have the same scale. Do this by creating a `StandardScaler()`, then run its `fit_transform` method on X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "# ----------\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train a projection method on the vectors, such as those methods offered in scikit-learn, then plot the projection as a scatter plot which we will do next.\n",
    "\n",
    "### Plot Word Vectors Using PCA <a id='subsection 3'></a>\n",
    "\n",
    "Recall that we can create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class. Construct a PCA objectusing the `PCA()` class of the scikit-learn library (setting n_components=2 so we can graph it in two dimensions) and use its fit_transform method on your standardized X to get Y_pca: the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a PCA\n",
    "# ----------\n",
    "pca = PCA(...)\n",
    "\n",
    "# fit and transform the standardized data\n",
    "# ----------\n",
    "Y_pca = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting projection can be plotted using `matplotlib`, pulling out the two dimensions as x and y coordinates. Create a scatter plot of the standardized word embeddings, setting the __size of each scatter point to 5__ to avoid overcrowding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "# ----------\n",
    "sns.scatterplot(x = ...,  # extract all the elements from the first column\n",
    "                y = ...); # extract all the elements from the second column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__QUESTION__: What do each point represent? What do the x and y axes represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER__: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might at this point still be confused on what the x- and y- axes represent. Because PCA selects and combines features according to what best describes and models the desired variable, the x and y axes actually **don't have an intuitive meaning on a human level.** PCA's job is to reduce the dimension of the features, and in this case it reduce the 100 features of each word vector to just the 2 that best described the words we modeled on. So, don't worry too much about what the coordinates of each word represents - we just want you to have a general and visual understanding of word vectors and how they may be related to one another on a graph.\n",
    "\n",
    "On that note, run the following cell. This will label each vector with its respective word. \n",
    "\n",
    "**CHALLENGE:** Annotate each line to ensure clear understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate visualization with points\n",
    "# ----------\n",
    "\n",
    "# ANNOTATE EACH LINE\n",
    "\n",
    "#\n",
    "import random\n",
    "from adjustText import adjust_text\n",
    "\n",
    "#\n",
    "random.seed(10)\n",
    "\n",
    "#\n",
    "rando = random.sample(list(model.wv.index_to_key), 25) \n",
    "\n",
    "#\n",
    "X1 = model.wv.__getitem__(rando)\n",
    "\n",
    "\n",
    "pca1 = PCA(n_components=2, \n",
    "           random_state=15)\n",
    "\n",
    "#\n",
    "result = pca1.fit_transform(X1)\n",
    "\n",
    "#\n",
    "result_df = pd.DataFrame(result,                   # \n",
    "                         columns = ['PC1', 'PC2'], # \n",
    "                         index = rando)            # \n",
    "\n",
    "#\n",
    "sns.scatterplot(x = 'PC1',         # \n",
    "                y = 'PC2',         # \n",
    "                data = result_df)  # \n",
    "\n",
    "#\n",
    "texts = []\n",
    "\n",
    "#\n",
    "for word in result_df.index:\n",
    "    texts.append(plt.text(result_df.loc[word, 'PC1'], \n",
    "                          result_df.loc[word, 'PC2'], \n",
    "                          word, \n",
    "                          fontsize = 8))\n",
    "    \n",
    "#\n",
    "adjust_text(texts, \n",
    "            force_text = (0.4,0.4),\n",
    "            expand = (1.2,1),\n",
    "            arrowprops = dict(arrowstyle = \"-\", \n",
    "                              color = 'black', \n",
    "                              lw = 0.5))\n",
    "\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "\n",
    "Another popular unsupervised method for summarizing and visualizing word embeddings is [t-distributed stochastic neighbor embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding). We won't get into the details here, but the basic method involves:\n",
    "\n",
    "1. Estimating the joint probability of the distance between each pair of points, assuming a Gaussian distribution.\n",
    "2. Project the data into 1-dimension, and then estimate the joint probability of the distance between each pair of points assuming a Student's t-distribution.\n",
    "3. Use gradient descent to update the second distribution to become similar to the first one.\n",
    "\n",
    "The basic idea behind this two-step procedure is that we search for the the best lower dimensional representation that gets closest to modeling the original distances in higher dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# ----------\n",
    "\n",
    "# filter to include only those for which Word2Vec has a vector\n",
    "vector_list = [model.wv.__getitem__(word) for word in words if word in model.wv.index_to_key]\n",
    "\n",
    "# create a list of the words corresponding to these vectors\n",
    "words_filtered = [word for word in words if word in model.wv.index_to_key]\n",
    "\n",
    "# bind together both lists using zip\n",
    "word_vec_zip = zip(words_filtered, vector_list)\n",
    "\n",
    "# create a dictionary and save as a dataframe\n",
    "word_vec_dict = dict(word_vec_zip)\n",
    "word_vec_df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
    "word_vec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize a t-SNE model using 2 components as the key hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create t-SNE visualization\n",
    "# ----------\n",
    "\n",
    "# initialize t-SNE\n",
    "tsne = TSNE(n_components = 2,  # specify 2 components\n",
    "            init = 'random',   # set initalization\n",
    "            random_state = 10, # set seed\n",
    "            perplexity = 100)  # set preplexity threshold\n",
    "\n",
    "# subset to only 400 rows to speed up training time\n",
    "tsne_df = tsne.fit_transform(word_vec_df[:400])\n",
    "\n",
    "# figure specifications\n",
    "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
    "sns.scatterplot(x = tsne_df[:, 0], \n",
    "                y = tsne_df[:, 1], \n",
    "                alpha = 0.5)\n",
    "\n",
    "# initialize empty list\n",
    "texts = []\n",
    "\n",
    "# create list of words\n",
    "words_to_plot = list(np.arange(0, 400, 10))\n",
    "\n",
    "# append words to list using loop\n",
    "for word in words_to_plot:\n",
    "    texts.append(plt.text(tsne_df[word, 0], \n",
    "                          tsne_df[word, 1], \n",
    "                          word_vec_df.index[word], \n",
    "                          fontsize = 10))\n",
    "    \n",
    "# adjust text to clearlly see labels\n",
    "adjust_text(texts, \n",
    "            force_text = (0.4,0.4),\n",
    "            expand = (1.2,1),\n",
    "            arrowprops = dict(arrowstyle = \"-\",\n",
    "                              color = 'black', \n",
    "                              lw = 0.5))\n",
    "\n",
    "# plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: t-SNE hyperparameters\n",
    "\n",
    "Try playing with the hyperparameters to see if you can get a different looking plot. Why might this be a problem for interpretability or inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate t-SNE visualization with 3 components\n",
    "# ----------\n",
    "tsne = TSNE(n_components = ..., # specify 3 components  \n",
    "            init = 'random',    # set initalization \n",
    "            random_state = 10,  # set seed   \n",
    "            perplexity = ...)   # you might have to lower preplexity threshold\n",
    "   \n",
    "\n",
    "# subset to only 400 rows to speed up training time\n",
    "tsne_df = tsne.fit_transform(word_vec_df...)\n",
    "\n",
    "\n",
    "\n",
    "# figure specifications\n",
    "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
    "sns.scatterplot(x = tsne_df[:, 0], \n",
    "                y = tsne_df[:, 2], \n",
    "                alpha = 0.5)\n",
    "\n",
    "\n",
    "# initialize empty list\n",
    "texts = ...\n",
    "\n",
    "# create list of words\n",
    "words_to_plot = list(np.arange(0, 400, 10))\n",
    "\n",
    "# append words to list using loop\n",
    "for word in words_to_plot:\n",
    "    texts.append(plt.text(tsne_df[word, 0], \n",
    "                          tsne_df[word, 1],\n",
    "                          word_vec_df.index[word], \n",
    "                          fontsize = 10))\n",
    "    \n",
    "# adjust text to clearlly see labels   \n",
    "adjust_text(texts, \n",
    "            force_text = (0.4,0.4),\n",
    "            expand = (1.2,1),\n",
    "            arrowprops = dict(arrowstyle = \"-\",\n",
    "                              color = 'black', \n",
    "                              lw = 0.5))\n",
    "\n",
    "# plot \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging Word Embeddings\n",
    "\n",
    "You'll notice that right now each token is represented by a 100-dimensional array. If we passed these directly to a classification algorithm our feature space would get very high dimensional quickly! A common practice to avoid this problem is to average the word embeddings somehow. You might have heard of variations of `word2vec` like `sent2vec` and `par2vec` which creates embeddings for sentences and paragraphs, respectively. We'll introduce a similar method below, but one straightforward way to do this without a fancy new model is to simply average the word embeddings at the document level.\n",
    "\n",
    "To get a sense of how this works, let's look at how many tokens we have in our first document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get length\n",
    "# ----------\n",
    "len(cfpb['tokens'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "223 - but remember not all documents will have vectors associated with them if do not meet word2vec's criteria. Let's see how many we have that are in our model's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model's vocabulary\n",
    "# ----------\n",
    "doc = [word for word in cfpb['tokens'][0] if word in model.wv.index_to_key]\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "195! Looks like quite a few tokens didn't make it into the model. Let's look at a few that did: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first five\n",
    "# ----------\n",
    "doc[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the array for 'contacting'. Notice that it is represented by a 100-dimensional array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embeddings for \"contacting\"\n",
    "# ----------\n",
    "print(model.wv.__getitem__('contacting'))\n",
    "print(model.wv.__getitem__('contacting').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mean\n",
    "# ----------\n",
    "np.mean(model.wv.__getitem__('contacting'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab the first vector each token and take their mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first vector of each token and find their mean\n",
    "# ----------\n",
    "# create empty list\n",
    "first_vec = ...\n",
    "\n",
    "# loop over each document\n",
    "for token in model.wv.__getitem__(doc):\n",
    "    first_vec.append(token[0])\n",
    "    \n",
    "# calculate their mean\n",
    "..(first_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then let's do this for every token and document in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to do this for every token and document in our corpus\n",
    "# ----------\n",
    "def document_vector(word2vec_model, doc):\n",
    "    doc = [word for word in doc if word in model.wv.index_to_key]\n",
    "    return np.mean(word2vec_model.wv.__getitem__(doc), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array for the size of the corpus\n",
    "# ----------\n",
    "# create empty list\n",
    "empty_list_embeddings_means = []\n",
    "\n",
    "# loop over each each token\n",
    "for puppy in cfpb['tokens']: # append the vector for each document\n",
    "    empty_list_embeddings_means.append(document_vector(model, puppy))\n",
    "    \n",
    "# convert the list to array\n",
    "doc_average_embeddings = np.array(empty_list_embeddings_means)\n",
    "\n",
    "# print averages\n",
    "doc_average_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately we get an array with `n` rows and 100 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the shape \n",
    "doc_average_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document averaged work embedding (doc2vec)\n",
    "\n",
    "Document averaged word embeddings tend to perform well with downstream prediction tasks, but there are other options as well. Here, we'll take a look at [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html). This is also a very helpful [tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html).  \n",
    "\n",
    "We're getting close to classifying, and this is a good point to do our train/test splits. While normally we recommend waiting to do splits until after all preprocessing is done, in this case it will be easier to do the split now because of the way the `TaggedDocument` class works. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# ----------\n",
    "\n",
    "# intitalize label binarizer\n",
    "lb_style = LabelBinarizer()\n",
    "\n",
    "# fit transform\n",
    "y = cfpb['Product_binary'] = lb_style.fit_transform(cfpb[\"Product\"])\n",
    "\n",
    "# train/test split\n",
    "train, test = train_test_split(cfpb,             # specify dataset\n",
    "                               test_size=0.2,    # specify test size\n",
    "                               random_state=42)  # set seed\n",
    "# view\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we do our train test splits, we apply the `TaggedDocument()` function to every token. This allows us to associate each document with the class that we want to predict later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tag to each train/test dataset\n",
    "# ----------\n",
    "\n",
    "# tag training datatset\n",
    "cfpb_train_tagged = train.apply(lambda r: TaggedDocument(words=r['tokens'],\n",
    "                                                         tags=[r.Product_binary]), \n",
    "                                axis=1)\n",
    "\n",
    "# tag testing datatset\n",
    "cfpb_test_tagged = test.apply(lambda r: TaggedDocument(words=r['tokens'], \n",
    "                                                       tags=[r.Product_binary]),\n",
    "                              axis=1)\n",
    "\n",
    "# view the first row\n",
    "cfpb_train_tagged[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to train our `doc2vec!` One of the key features of gensim is that it natively allows multicore processing - meaning it can take advantage of your CPU cores. Note that this is slightly different from tensorflow that we covered last semester, which also allows GPU acceleration. You can check how many CPU cores you have available: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count your cores for processing\n",
    "# ----------\n",
    "cores = multiprocessing.cpu_count()\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Parallel processing](https://en.wikipedia.org/wiki/Parallel_computing) is an important topic in computational social science - it will be the key to speeding up lots of different operations. In general, we recommend that whenever you use parallel processing you reserve 1 CPU core for your computer's other functions (keeping your browser and other software running), and use the remaining for your task at hand. In this case, doc2vec's training process is an example of [embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel) meaning that the different CPUs don't need to talk to each other to do their computations. They can independently work and combine the results at the end. In this case, we are working with very few observations (just 1000), but this is a useful technique to keep in mind for your projects and research! \n",
    "\n",
    "First we'll train the model (notice the use of the [`tqdm`](https://tqdm.github.io/) library for progerss bars):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a Doc2Vec model \n",
    "# ----------\n",
    "# initalize Doc2Vec\n",
    "model_dbow = Doc2Vec(...,               # specify a distributed bag of words\n",
    "                     ...,               # set word embedding to 300\n",
    "                     ...,               # include 5 negative samples\n",
    "                     hs=0,              # hierarchical softmax using negative samples\n",
    "                     min_count=2,       # ignores all words with a total frequency lower than this threshold.\n",
    "                     ...,               # essentially turn off downsampling\n",
    "                     seed = 1995,       # set seed for reproducibility\n",
    "                     ...)               # set # of cores to 1 less than you have - not be fully reproducible if not 1\n",
    "\n",
    "# apply to training data\n",
    "model_dbow.build_vocab([x for x in tqdm(cfpb_train_tagged.values)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll allow the model to train for 30 iterations (epochs - this is the same as our neural nets lab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over our data\n",
    "# ----------\n",
    "for epoch in range(1,30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(cfpb_train_tagged.values)]), \n",
    "                     total_examples=len(cfpb_train_tagged.values), \n",
    "                     epochs=epoch)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll define a function that will grab the embeddings for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the embeddings\n",
    "# ----------\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "And finally, let's use logistic regression to see how well our document embeddings do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Classifcation model\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "# intialize a logit model\n",
    "# ----------\n",
    "\n",
    "# split into training\n",
    "y_train, X_train = vec_for_learning(model_dbow,     # specify datasets for split\n",
    "                                    cfpb_train_tagged)\n",
    "\n",
    "# split into testing\n",
    "y_test, X_test = vec_for_learning(model_dbow,       # specify datasets for split\n",
    "                                  cfpb_test_tagged)\n",
    "\n",
    "# initalize model\n",
    "logit_reg = ...()  # initialize logit model\n",
    "\n",
    "# fit on training\n",
    "logit_model = logit_reg.fit(...,   # fit to training data\n",
    "                            ...)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = logit_model.predict(...) # predict on test data\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "# ----------\n",
    "\n",
    "# create confusion matrix \n",
    "cf_matrix = confusion_matrix(...,                 # actual\n",
    "                             ...,                 # predictions\n",
    "                             ...)                 # normalize\n",
    "\n",
    "# create dataframe\n",
    "df_cm = pd.DataFrame(....,    # specify matrix for calculations \n",
    "                     range(2),\n",
    "                     range(2))\n",
    "\n",
    "# rename indices\n",
    "df_cm = df_cm.rename(index=str, columns={0: \"Checking or savings account\", \n",
    "                                         1: \"Student loan\"})\n",
    "\n",
    "df_cm.index = [\"Checking or savings account\", \"Student loan\"]\n",
    "\n",
    "# plot specifications\n",
    "# ----------\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, not as well as we might think! But this shouldn't be too surprising with a small dataset - word embeddings tend to work best when given lots of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pre-Trained Embeddings\n",
    "\n",
    "So far we have been working with embeddings trained on our particular corpus. However, this is not usually standard - as you saw above, word2vec works best when it has lots of data. The problem is that training state-of-the-art models requires intense computational resources. It also has a [large carbon footprint](https://arxiv.org/pdf/1906.02243.pdf). Luckily, we can use pre-trained models like Google News or Stanford's GloVe. Note to run this next chunk of code, you need to have the [GoogleNews embeddings](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) in your \"data\" directory in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained Google News model \n",
    "# ----------\n",
    "googlenews_word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('../../data/GoogleNews-vectors-negative300.bin.gz', \n",
    "                                                                            binary = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also tune the Google News embeddings to a domain-specific corpus. This may or may not be necessary depending on how specific or unique you think words in your corpus might be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain google on your own corpus\n",
    "# ----------\n",
    "\n",
    "# ! NOTE: THIS TAKES A VERY LONG TIME TO RUN - SO WE WON'T RUN IT HERE\n",
    "# ! BUT WE CAN RE-RUN FROM HERE TO SEE HOW THIS MIGHT ULTIMATELY AFFECT\n",
    "# ! OUR CLASSIFICATION MODEL\n",
    "\n",
    "\n",
    "\n",
    "## specify your corpus\n",
    "#your_corpus = cfpb['tokens']  # Load your corpus here, make sure it's tokenized\n",
    "#\n",
    "## initialize Word2Vec model with the same dimensions as the Google News vectors\n",
    "#word2vec_model = gensim.models.Word2Vec(vector_size=300,    # word embedding size\n",
    "#                                        window=5,           # window size\n",
    "#                                        min_count=1,        # ignores words w/ frequency lower than this threshold\n",
    "#                                        workers=cores - 1)  # how many cores will be used\n",
    "#\n",
    "## build vocabulary \n",
    "#word2vec_model.build_vocab(your_corpus)\n",
    "#\n",
    "## initialize word vectors using the Google News pre-trained vectors\n",
    "#word2vec_model.wv.vectors = googlenews_word2vec_model.vectors\n",
    "#\n",
    "## training model\n",
    "#word2vec_model.train(your_corpus, \n",
    "#                     total_examples=len(your_corpus), \n",
    "#                     epochs=5, \n",
    "#                     compute_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "### Challenge Embeddings \n",
    "\n",
    "Repeat the exploration we did in the first part where we trained word embeddings with the new Google model. Do you notice any differences? Create document averaged word embeddings and predict our outcome ('Product_binary'). How did this compare to doc2vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return embeddings for specific word -- \"account\"\n",
    "# ----------\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Check out the shape of the vectors for 'account', what do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape of the embeddings - what does this tell us?\n",
    "# ----------\n",
    "googlenews_word2vec_model.__getitem__(['account']). ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following empty cells to look at what the word embeddings look like for words you think may appear in the text! Keep in mind that even if a word shows up in the text as seen above, a word vector will not be created unless it satisfies all conditions we inputted into the model above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word 1 - try the word \"navient\"\n",
    "# ----------\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Did this run as you expected? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word 2 - try the word \"company\"\n",
    "# ----------\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word 3 - try the word \"credit\"\n",
    "# ----------\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a list of words for which we have calculations\n",
    "# ----------\n",
    "words = list(googlenews_word2vec_model. ...) # get the indices and keys\n",
    "print(words...)                              # return the first 100 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gensim` comes with some handy methods to analyze word relationships. `similarity` will give us a number from 0-1 based on how similar two words are. If this sounds like cosine similarity for words, you'd be right! It just takes the cosine similarity of the high dimensional vectors we input. \n",
    "\n",
    "In the following cell, find the similarity between the words `credit` and `debt`. \n",
    "\n",
    "**QUESTION:** How does this compare to the model trained on our own small dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarty between credit and debt and compare to above\n",
    "# ----------\n",
    "googlenews_word2vec_model...(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** ...\n",
    "\n",
    "We can also find cosine distance between two clusters of word vectors. Each cluster is measured as the mean of its words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between credit/debt and loan/mortgage\n",
    "# ----------\n",
    "googlenews_word2vec_model.n_similarity(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find words that don't belong with `doesnt_match` parameter. This finds the mean vector of the words in the `list`, and identifies the furthest away. Try it out. Of the three words in the list `['credit', 'loan', 'student']`, which is the furthest vector from the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words that don't match 1\n",
    "# ----------\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words that don't match 2\n",
    "# ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the Google News model did for this example above: \n",
    "$$\\vec{house} - \\vec{rent} + \\vec{loan} = $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector math with google\n",
    "# ----------\n",
    "googlenews_word2vec_model.most_similar(positive=..., \n",
    "                                       negative=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge PCA\n",
    "\n",
    "Next we will explore the word embeddings of our `text` visually with PCA. We can retrieve __all__ of the vectors from a trained model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all vectors\n",
    "# ----------\n",
    "\n",
    "# ANNOTATE WHAT EACH LINE IS DOING\n",
    "\n",
    "# \n",
    "model_words = [word for word in doc if word in model.wv.index_to_key and word in googlenews_word2vec_model.index_to_key]\n",
    "\n",
    "# \n",
    "X = googlenews_word2vec_model.__getitem__(model_words)\n",
    "\n",
    "#\n",
    "print(X.shape) # what is the shape of this and what does it mean?\n",
    "print(model_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we do with non-text features, we want to standardize X so that all features have the same scale. Do this by creating a `StandardScaler()`, then run its `fit_transform method` on X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "# ----------\n",
    "X_std = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a PCA \n",
    "# ----------\n",
    "# initalize model and set parameters\n",
    "pca = ... # set n_components to 2 \n",
    "\n",
    "# fit and transform the standardized data\n",
    "Y_pca = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "# ----------\n",
    "sns.scatterplot(x = ...[:, 0], # extract all the elements from the first column\n",
    "                y = ...[:, 1]) # extract all the elements from the second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the visualization with 2 components\n",
    "# ----------\n",
    "\n",
    "# set random seed\n",
    "random.seed(10)\n",
    "\n",
    "# sample 25 words\n",
    "rando_words = random.sample(model_words, 25) \n",
    "\n",
    "# get embeddings\n",
    "X1 = model.wv.__getitem__(rando_words)\n",
    "\n",
    "# initalize PCA\n",
    "pca1 = PCA(n_components=2, \n",
    "          random_state=16)\n",
    "\n",
    "# fit and transform\n",
    "result = pca1.fit_transform(X1)\n",
    "\n",
    "# create dataframe\n",
    "result_df = pd.DataFrame(result,               \n",
    "                         columns = ['PC1', 'PC2'], \n",
    "                         index = rando)\n",
    "\n",
    "# create scatterplot\n",
    "sns.scatterplot(x = 'PC1',        # specify x-axis\n",
    "                y = 'PC2',        # specify y-axis\n",
    "                data = result_df) # specify dataset\n",
    "\n",
    "# initialize empty list\n",
    "texts = []\n",
    "\n",
    "# append words to list - FOR SOME REASON, THIS LINE WILL NOT RUN. YOU GET THE SAME OUTPUT JUST W/O THE LABELS\n",
    "#for word in result_df.index:\n",
    "#    texts.append(plt.text(result_df.loc[word, 'PC1'], \n",
    "#                          result_df.loc[word, 'PC2'], \n",
    "#                          word, \n",
    "#                          fontsize = 8))\n",
    "\n",
    "# plot text using adjust_text (because overlapping text is hard to read)\n",
    "adjust_text(texts, \n",
    "            force_text = (0.4,0.4),\n",
    "            expand = (1.2,1),\n",
    "            arrowprops = dict(arrowstyle = \"-\", \n",
    "                              color = 'black', \n",
    "                              lw = 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging Word Embeddings\n",
    "\n",
    "And then let's do this for every token and document in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to iterate over every token and document in our corpus\n",
    "# ----------\n",
    "def document_vector(model, doc):\n",
    "    doc = [word for word in doc if word in model.index_to_key]\n",
    "    return np.mean(model.__getitem__(doc), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array for the size of the corpus\n",
    "# ----------\n",
    "# ANNOTATE EACH LINE\n",
    "\n",
    "#\n",
    "empty_list_embeddings_means = []\n",
    "\n",
    "#\n",
    "for doc in cfpb['tokens']: \n",
    "    empty_list_embeddings_means.append(document_vector(googlenews_word2vec_model, doc))\n",
    "\n",
    "# \n",
    "doc_average_embeddings = np.array(empty_list_embeddings_means) \n",
    "\n",
    "# \n",
    "doc_average_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Classification\n",
    "\n",
    "Let's run a classificaiton model using the Google News trained model. \n",
    "\n",
    "**QUESTION:** How does this pre-trained library do compared to our first model fit to our own data? Why do you think this is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert word embeddings into dataframe\n",
    "# ----------\n",
    "word2vec_features_df = pd.DataFrame(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Classifcation model\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "# specify logit model\n",
    "# ----------\n",
    "\n",
    "# create label\n",
    "y = ...  # subset the product binary column from cfpb dataframe\n",
    "\n",
    "# split into training\n",
    "X_train, X_test, y_train, y_test = train_test_split(...,                # specify features\n",
    "                                                    ...,                # specify labels\n",
    "                                                    ... = ...,          # specify training split\n",
    "                                                    ...=...,            # specify test split\n",
    "                                                    random_state = 10)  # set seed\n",
    "# inititialize a model\n",
    "logit_reg = LogisticRegression()\n",
    "\n",
    "# fit on training\n",
    "logit_model = logit_reg.fit(..., \n",
    "                            ....ravel())\n",
    "\n",
    "# predict on test data\n",
    "y_pred = logit_model...(...)\n",
    "\n",
    "# confusion matrix\n",
    "# ----------\n",
    "\n",
    "# create confusion matrix \n",
    "cf_matrix = ....(...,              # actual\n",
    "                 ...,              # predictions\n",
    "                 ... = \"true\")     # normalize\n",
    "\n",
    "# create dataframe\n",
    "df_cm = pd.DataFrame(...,   # specify matrix for calculations\n",
    "                     range(2),\n",
    "                     range(2))\n",
    "\n",
    "# rename indices\n",
    "df_cm = df_cm.rename(index=str, columns={0: \"Checking or savings account\", \n",
    "                                         1: \"Student loan\"})\n",
    "\n",
    "df_cm.index = [\"Checking or savings account\", \"Student loan\"]\n",
    "\n",
    "\n",
    "# plot specifications\n",
    "# ----------\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "plt.title('Google Document Averaged Embeddings')\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** ...\n",
    "\n",
    "Uncomment the script code above to re-train the Google News model on your data and re-run to see how it performs in comparison. How do you think it will do?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "You've now had a gentle introduction to word embeddings! There are a few major lessons here.\n",
    "\n",
    "- Word embeddings are powerful and capture a lot of context that frequency based embeddings do not. However, this isn't perfect! \n",
    "- As with any machine learning application, your choice of model and hyperparameters can matter quite a lot. In this case, some of our simpler featurizations and models actually did better than word embeddings, but this won't always be true. \n",
    "- It is also worth learning more about other embeddings like [GloVe](https://nlp.stanford.edu/projects/glove/), transformer based models like [BERT](https://arxiv.org/abs/1810.04805) and deep learning approaches like [ELMo](https://arxiv.org/abs/1802.05365)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by Aniket Kesari. Some materials borrowed from [LS123: Data, Prediction, and Law](https://github.com/Akesari12/LS123_Data_Prediction_Law_Spring-2019/blob/master/labs/Word%20Embedding/LEGALST-190%20Word%20Embedding%20SOLUTIONS.ipynb). Modified by Prashant Sharma (2023) and annotated by Kasey Zapatka (2024)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
