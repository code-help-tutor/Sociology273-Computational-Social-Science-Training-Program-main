{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlab-berkeley/Computational-Social-Science-Training-Program/blob/master/Deep_Learning_and_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DrTKxZ1hCS5"
      },
      "source": [
        "# [Computational Social Science]\n",
        "## 5-5 Deep Learning - Student Version\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHoISf71wzH2"
      },
      "source": [
        "This notebook will introduce you to the fundamentals of Keras and explore techniques for deep learning with text data. Key concepts covered in this notebook include:\n",
        "\n",
        "1. Google Colab and GPUs\n",
        "2. Use keras to adapt and tune neural nets\n",
        "3. Existing resources to help analyze language data\n",
        "\n",
        "\n",
        "With these basic building blocks, you will be equipped to explore and implement deep learning algorithms for your own project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNqj6s9msZga"
      },
      "source": [
        "# Google Colab\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNovoJcyugcM"
      },
      "source": [
        "Objectives:\n",
        "\n",
        "- Set up a Google Colab notebook\n",
        "- Create, delete, run, and edit cells\n",
        "- Cover variable, notebook and package management\n",
        "\n",
        "15 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_zANHRDhplZ"
      },
      "source": [
        "## Introducing Google Colab\n",
        "\n",
        "\n",
        "Google Colab is a platform for cloud-based computation and coding. It is similar to a Jupyter Notebook, where individual cells of code are executed sequentially. It doesn't require local installation on your computer and can be shared and edited by multiple people at the same time. However the Colab notebook requires you to be connected to the internet, while jupyter notebooks can be run on your local machine. Google Colab notebooks are in the .ipynb format, and can be saved and opened either directly or via Google Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqx972vJvdVv"
      },
      "source": [
        "##Basic Operations\n",
        "\n",
        "Google Colab has several features that help organize code and long notebooks. A few key concept to know to use this notebook effectively are:\n",
        "\n",
        "- Use the Insert tab in the upper bar, or press the +Code/+Text buttons in the top left of the window.\n",
        "\n",
        "- Text cells can be edited and formatting with the buttons at the top of the cell.\n",
        "\n",
        "- The buttons at the top right of the cell give you options to move, modify and delete the cell.\n",
        "\n",
        "- You can run code with shift+enter, or by clicking the top left of the box.\n",
        "\n",
        "- For more commands, use ctl+shift+p and select the desired command from the command palette\n",
        "\n",
        "An example code cell is below. Try executing and editing the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhl6KsHCrLP-"
      },
      "outputs": [],
      "source": [
        "print(\"Welcome to Google Colab\")\n",
        "x=12+78"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT9L_Th9sPrV"
      },
      "source": [
        "The buttons on the left panel help manage the notebook (search, table of contents, files). This is important for organizing your code and navigating long notebooks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR9JpuTHcmVz"
      },
      "source": [
        "## Package Management\n",
        "\n",
        "Like Anaconda, Google Colab comes with many packages already available, and you can also install local packages using pip. Use the following lines of code in order to see which packages you have and which ones you need to install.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#check which packages you have available (listed alphabetically). The version numbers are also avaliable which can be useful in determining issues with coding between computers.\n",
        "!pip list\n",
        "\n",
        "#install a new package\n",
        "!pip install numpy\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<List of packages that we will use in this tutorial>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ3GQZB7bo6M"
      },
      "source": [
        "In the following cell are the packages that you will need to complete this notebook. Run this line of code to make sure that everything is installed properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qey74xT0boGX"
      },
      "outputs": [],
      "source": [
        "#import packages for deep learning\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "#from keras.utils import np_utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi2ikVCaoNct"
      },
      "source": [
        "## Customization\n",
        "\n",
        "Finally, there are several settings that you can customize if you so choose. These can be found under Tools -> Settings, where you can change the font size, background, and other aesthetic settings of the notebook to suit you.\n",
        "\n",
        "In addition, in Tools -> Keyboard shortcuts you can view and adapt shortcuts to your preferences as well.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHrAjcLHsYC3"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Try out the following exercises to get comfortable with the new interface:\n",
        "\n",
        "1) Open the editor settings (Tools-> Settings->Editor) and select \"Show line numbers\". Now your cells will have line numbers next to them, which we can refer to when discussing code during this workshop.\n",
        "\n",
        "2) Make a new code cell below and save the product of 60 and 72 to a new variable. Then check the value of the variable in the variable tab to the left.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2iLcnJj1-Ng"
      },
      "outputs": [],
      "source": [
        "#your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A58nCiMsshpn"
      },
      "source": [
        "# Introduction to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1AkXBaoujUc"
      },
      "source": [
        "Objectives:\n",
        "- Understand the benefits of GPUs\n",
        "- Set up GPU for Google Colab\n",
        "- Compare performance on tasks vs CPU\n",
        "\n",
        "\n",
        "10 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp2UXU5HHTSM"
      },
      "source": [
        "As you've found in your previous models, some models take a significant amount of time to run. Models may also exceed the capacity of the local computer's processing power. This will either result in code that never finished running, or an error message indicating that the code has timed out without completing.\n",
        "\n",
        "To counteract this issue, TPU/GPU are parallel processing units that greatly speed up models. This can make some models that are otherwse impossible to train possible (Think minutes rather than hours)\n",
        "\n",
        "TPU is made specifically for tensorflow architecture, and speeds it up even more than GPUs.\n",
        "\n",
        "## GPU Access\n",
        "Oftentimes you need to pay for cloud services and access to GPUs, but one advantage of Colab is that it has free access to a certain amount of GPU/TPU units. This access is somewhat limited, but should be more than enough for what we are using it for today. We will discuss limitations and further options for long-term use in a later section of the workshop.\n",
        "\n",
        "\n",
        "Additional resource: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=sXnDmXR7RDr2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co9jxID0Utsj"
      },
      "source": [
        "The notebook will automatically choose which device (read: GPU vs CPU) to run the code on, but if you want to make sure that something is being run on a certain device, you can select a specific device as in the snippet below.\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "with tf.device(device_name):\n",
        "  #put task here\n",
        "  #return output\n",
        "```\n",
        "\n",
        "For now, we will trust the notebook's/ Tensorflow's allocation of computing power.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9xyYBVVRZlE"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "1) Run the following lines of code to test how fast your computer can do a task. Report the results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqgQQ2a1SBDj"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "\n",
        "print(timeit.timeit('[x**2 for x in range(10)]'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx9qArhdR-31"
      },
      "source": [
        "2)  Change the settings to use GPU:  Edit --> Notebook Settings --> Hardware Accelerator --> GPU\n",
        ". Run the code below to make sure GPU is enabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow0GhY3ANepm"
      },
      "outputs": [],
      "source": [
        "#run this code to check that you have the GPU enabled\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6kl-9FvVbBD"
      },
      "source": [
        "3) Re-run the same timing task and report your results. How much of a difference is there in timing?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9ADVC5SVabz"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "print(timeit.timeit('[x**2 for x in range(10)]'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GvtUiHVV6IO"
      },
      "source": [
        "As we run more complex tasks, the efficiency of GPUs becomes more and more of a difference. If you are curious, you can compare the timing of the tasks in this notebook with GPU/TPU/CPU and note the difference. Even though in this notebook we are working with fairly small dataset and task, these differences will be important at larger scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TR78IdDij7M"
      },
      "source": [
        "# Deep Learning\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6twFHgyZSeiv"
      },
      "source": [
        "Objectives:\n",
        "- Code and optimize a neural network\n",
        "- Adapt a network to new data\n",
        "\n",
        "20 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRq8wjQaqiWW"
      },
      "source": [
        "In previous sections of this course, you have covered neural networks and deep learning for classifying the MNIST dataset. The task was classifying handwritten digits 0-9 based on images. In this section, we will revisit deep learning in Python with text data.\n",
        "\n",
        "We will start with the classificaton problem (student loan vs checking/savings account) from the NLP section of the course, where customer complaint data was used classify what type of account the complaint was related to. We will use the same embeddings we trained for the final logistic regression problem in that section of the course.\n",
        "\n",
        "First, we will load in the data and split it into training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMyrg9cv0pDS"
      },
      "outputs": [],
      "source": [
        "word2vec_features_df=pd.read_csv('https://github.com/dlab-berkeley/Computational-Social-Science-Training-Program/raw/main/data/embeddings.csv').iloc[:, 1:]\n",
        "y=pd.read_csv('https://github.com/dlab-berkeley/Computational-Social-Science-Training-Program/raw/main/data/y.csv')\n",
        "y_vals=y['Product_binary'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(word2vec_features_df,\n",
        "                                                    y_vals,\n",
        "                                                    train_size = .80,\n",
        "                                                    test_size=0.20,\n",
        "                                                    random_state = 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhyFnEKUh_uz"
      },
      "source": [
        "We will use Keras now. Keras is a widely-used high-level neural network API known for its simplicity and quick prototyping capabilities. While it was originally an independent project that could operate with various backends such as Theano and CNTK, TensorFlow has emerged as its primary backend. This integration allows Keras to take advantage of TensorFlow's powerful features, including its ability to run smoothly on different hardware like CPUs, GPUs, and TPUs. By utilizing TensorFlow as its underlying engine, Keras simplifies many complex aspects of deep learning, making the development of neural networks more accessible. Its user-friendly nature has led to extensive use in both industrial applications and academic research.\n",
        "\n",
        "Next we define the model. In Keras, each layer of the model has to be individually specified. This allows significant control over the model, including different parameters for each level.\n",
        "\n",
        "This model has a dense layer with 128 neurons in each, and a dropout layer where 20% of the connections are dropped out for each layer. The final output layer uses a sigmoid activation function to create a final binary output (0 or 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "colGv5dcuzwP"
      },
      "outputs": [],
      "source": [
        "def NN_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "\n",
        "    # A fully connected layer with 128 neurons\n",
        "    model.add(Dense(128, input_dim=300,activation='relu'))\n",
        "\n",
        "    # A dropout layer that randomly excludes 20% of neurons in the layer\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # An output layer with binary classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile model with crossentropy\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wH5KqiKiQQl"
      },
      "source": [
        "Finally, we fit and evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9AXKsZQwVsR"
      },
      "outputs": [],
      "source": [
        "model = NN_model()\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=0)\n",
        "\n",
        "# Evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"NN Error: %.2f%%\" % (100-scores[1]*100))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIN6sK82FQxp"
      },
      "source": [
        "This is a simple neural network with a couple of densely connected layers and a couple of dropout layers. When working with neural nets, it's often a good idea to start with a simple net to make sure the basics of the code work, then gradually create more complicated architectures once the code runs smoothly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGLS9pEUibTB"
      },
      "source": [
        "Now, let's use our tensor knowledge to adapt this architecture to another set of data. First, let's load in the MNIST digits dataset (in practice, we would likely be using a dataset more similar to the one in the original model). The MNIST dataset is three dimensions (n_samplesx28x28), so we need to flatten the data for now to create a two dimensional tensor  n_samplesx784 to fit with the neural net we are working on. Note: instead of two classes, the MNIST dataset uses 10 classes (one for each digit 0-9)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZTIeDTnBn8o"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "# reshape to [samples][width][height][pixels]\n",
        "X_train = X_train.reshape(X_train.shape[0], 28*28)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28*28)\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bP926VQjIJ6"
      },
      "source": [
        "Here is the same code from the NN model above. What do you need to change in order to run the same model on the new data? Note which parameters and values you need to change. How does this relate to the differences in the data? Let's edit the code to work with the new data shape and execute it.\n",
        "\n",
        "Hint: use tf.shape() to see the compare the shapes of the MNIST and original dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DtWkyXLfNbk"
      },
      "source": [
        "These are the lines of code we need to change to make this model work with new data:\n",
        "\n",
        "\n",
        "In line 6:\n",
        "```\n",
        "model.add(Dense(128, input_dim=784,activation='relu')) #change input dim\n",
        "```\n",
        "The embeddings dataset had 300 features, or columns, the new MNIST dataset has 784, so we need to make sure to match the numbers in model architecture.\n",
        "\n",
        "In line 12:\n",
        "\n",
        "```\n",
        "    model.add(Dense(10, activation='softmax')) #change dimension to number of categories\n",
        "```\n",
        "The final layer needs to have 10 categories, rather than two, since there are more classes in the MNIST dataset. In addition, the activation function needs to be changed to softmax.\n",
        "\n",
        "In lne 15:\n",
        "\n",
        "```\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "```\n",
        "\n",
        "Again, because of the number of classes, the loss function used must be categorical cross entropy rather than binary cross entropy.\n",
        "\n",
        "Here is the updated model:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsgLoMccgRtr"
      },
      "outputs": [],
      "source": [
        "def diff_CNN_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(128, input_dim=784,activation='relu')) #change input dim\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(.2))\n",
        "\n",
        "    model.add(Dense(10, activation='softmax')) #change dimension to number of categories and activation function\n",
        "\n",
        "    #change to categorical crossentropy\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = diff_CNN_model()\n",
        "# Fit the model\n",
        "print(X_train.shape)\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "# Evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"NN Error: %.2f%%\" % (100-scores[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23KhRlCwGLSi"
      },
      "source": [
        "# Optimizing Neural Nets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOPuwHOBSt2l"
      },
      "source": [
        "Objectives:\n",
        "- Explore strategies to optimize a neural net\n",
        "- Implement an optimizer with custom settings\n",
        "- Grid search parameters\n",
        "\n",
        "20 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZjXK0-OGT5C"
      },
      "source": [
        "Optimizing neural nets is a key point of using these powerful models effectively, as with any ML models. However, neural nets have many parameters that can be tuned and are a challenge for traditional optmization methods such as grid search.\n",
        "\n",
        "In the previous challenge, we experimented with improving the accuracy of the model. The following strategies can help guide the optmization process for fine-tuning algorithms.\n",
        "\n",
        "1. Feature engineering (refer to Natural Language Processing Notebook)\n",
        "\n",
        "2. Try a smaller network (minimize redundancy) or a larger network (capture more complex relationships)\n",
        "\n",
        "3. Change learning rate\n",
        "4. Use appropriate architecture for the data/task\n",
        "\n",
        "5. Test parameters\n",
        "\n",
        "6. Decrease batch size\n",
        "\n",
        "Depending on the task, data, and neural network used, there may be a significant amount of tuning necessary in order to achieve an optimal result. This is one reason why leveraging existing models that are already optimized can give a huge advantage for language tasks.\n",
        "\n",
        "Further reference this article: https://towardsdatascience.com/optimizing-neural-networks-where-to-start-5a2ed38c8345\n",
        "\n",
        "\n",
        "For this notebook we will start with changing the learning rate.\n",
        "\n",
        "In previous examples, we passed the optimizer to the compile funciton\n",
        "```\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "```\n",
        "Which uses the default parameters for the function. Now that we are customizing the parameters, we want to use the actual optimizer function, and then pass that optimizer into the .compile() function.\n",
        "\n",
        "```\n",
        "model.compile(....,opt=keras.optimizers.Adam())\n",
        "```\n",
        "\n",
        "Here is the documentation for that function: https://keras.io/api/optimizers/adam/\n",
        "\n",
        "What is the default parameter for learning rate? What are some of the other parameters for the Adam optimizer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElzhbM-eOPDe"
      },
      "source": [
        "##Challenge\n",
        "\n",
        "Test the following learning rates: [.0001,.001,.01,.1]. Which one performs the best? Which one performs the worst?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DZBPIH2OObp"
      },
      "outputs": [],
      "source": [
        "#load in data to use for this test\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "#cnn classification for neural nets\n",
        "word2vec_features_df=pd.read_csv('https://github.com/dlab-berkeley/Computational-Social-Science-Training-Program/raw/main/data/embeddings.csv').iloc[:, 1:]\n",
        "\n",
        "y=pd.read_csv('https://github.com/dlab-berkeley/Computational-Social-Science-Training-Program/raw/main/data/y.csv')\n",
        "y_vals=y['Product_binary'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(word2vec_features_df,\n",
        "                                                    y_vals,\n",
        "                                                    train_size = .80,\n",
        "                                                    test_size=0.20,\n",
        "                                                    random_state = 10)\n",
        "#print(word2vec_features_df.shape)\n",
        "#print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Trjy8DMSQCGj"
      },
      "outputs": [],
      "source": [
        "model = NN_model()\n",
        "# Fit the model\n",
        "print(X_train.shape)\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "# Evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AXRqpHOqKxy"
      },
      "source": [
        "# Challenge: Optimizing a Neural Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZj0IiVYjYtY"
      },
      "source": [
        "\n",
        "The logit model from the challenge question in NLP section used to classify the customer complaint data had an accuracy of 78.5%. What is the accuracy of the first neural network model on the same data? Hint: (read the output) Try changing the model to improve accuracy. What configuration gave you the best results? Try changing the parameters of the existing layers, or adding more layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSEW7T0Vpacv"
      },
      "outputs": [],
      "source": [
        "word2vec_features_df=pd.read_csv('https://github.com/dlab-berkeley/Computational-Social-Science-Training-Program/raw/main/data/embeddings.csv').iloc[:, 1:]\n",
        "y=pd.read_csv('https://github.com/dlab-berkeley/Computational-Social-Science-Training-Program/raw/main/data/y.csv')\n",
        "y_vals=y['Product_binary'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(word2vec_features_df,\n",
        "                                                    y_vals,\n",
        "                                                    train_size = .80,\n",
        "                                                    test_size=0.20,\n",
        "                                                    random_state = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpMjqhdt3HKh"
      },
      "outputs": [],
      "source": [
        "#original model\n",
        "\n",
        "def NN_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "\n",
        "    # A fully connected layer with 128 neurons\n",
        "    model.add(Dense(128, input_dim=300,activation='relu'))\n",
        "\n",
        "    # A dropout layer that randomly excludes 20% of neurons in the layer\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # An output layer with binary classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile model with crossentropy\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "    model = NN_model()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=0)\n",
        "\n",
        "# Evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"NN Error: %.2f%%\" % (100-scores[1]*100))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZEJquJOwESl"
      },
      "source": [
        "In practice we often take advantage of existing code and architecture to help accomplish deep learning tasks. This can range from taking a neural network architecture and adapting it to new data (as in our exercise above) to using other packages with pre-trained models. In the next section we will explore one such package called Huggingface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Ovl5RF2DjS"
      },
      "source": [
        "# Huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRXZt7DhS7mR"
      },
      "source": [
        "Objectives:\n",
        "- Explore tasks and data available in Huggingface transformers\n",
        "- Choose an appropriate language task\n",
        "- Implement a transformer on local data\n",
        "\n",
        "20 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glKdUE_CEw7h"
      },
      "source": [
        "In reality, these models  require significant data and computational power, which can exceed the resources available to the analyst. We can circumvent this problem by using pre-trained models. Like a pre-trained embedding model, pre-trained models are trained on a large dataset. While this may not perfectly align with the data or task you have, it can help create a more robust system that can be fine-tuned to your data and goals.\n",
        "\n",
        "[Huggingface](https://huggingface.co/models) is a set of pretrained models from a variety of datasets and sources with an easy-to-use interface. In this section, we will explore the use of the Huggingface library to streamline language task processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE52xADz3Py-"
      },
      "outputs": [],
      "source": [
        "#install the transformers library\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WygeyX2MQo8T"
      },
      "source": [
        "The simplest strategy is to use the pipeline method, where you select the task and the pre-trained model (there are multiple models available for many of the tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9pq4nvXSnMh"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdXQGdLgQ9sa"
      },
      "source": [
        "The key to using these models, since the preprocessing is built in, is understanding the format of the data necessary for the model. This model takes the raw text as input rather than the word embeddings, so let's reload our data appropriately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iahhf3AEUGBN"
      },
      "outputs": [],
      "source": [
        "cfpb=pd.read_csv('https://raw.githubusercontent.com/dlab-berkeley/Computational-Social-Science-Training-Program/main/data/CFPB%202020%20Complaints.csv')\n",
        "complaints=cfpb['Consumer complaint narrative']\n",
        "complaints=complaints[~complaints.isna()]\n",
        "classifier(complaints.values[0])\n",
        "print(complaints.values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_rQGYoPRNon"
      },
      "source": [
        "Then use the pipeline on the example data, and look at the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Thjw7tTaRSgH"
      },
      "outputs": [],
      "source": [
        "for k in range(10):\n",
        "  print(complaints.values[k])\n",
        "  print(classifier(complaints.values[k]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGFiOhimXOYq"
      },
      "source": [
        "As you might expect, the complaints dataset has mostly negative values. While this is somewhat of a trivial example, it highlights how in just a few lines of code and no preprocessing we can implement a model on our own data. While this doesn't work for every task, for example the specific classification task that we were working with above, this is a valuable and powerful tool for quick, out-of-the-box models that don't take very long to initialize and tune."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBrMsnXcJLxI"
      },
      "source": [
        "# Large Language Model\n",
        "We can use a large language model such as GPT-2 or GPT-Neo with the Hugging Face Transformers library. In this example, we will generate text using the GPT-2 model with the Hugging Face Transformers library. This example will use the code below to produce five different continuations of the provided input text.\n",
        "\n",
        "We will use pipeline function. The pipeline function is a simple way to create a model with its associated preprocessing and postprocessing steps.  It's a high-level function provided by the Hugging Face Transformers library that makes it easy to load a pre-trained model with its associated tokenizer and use it for various tasks like text generation, sentiment analysis, and more. Using the pipeline function is not strictly necessary, but it does simplify the code by abstracting away several underlying details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkoC7cRRJLxI"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "# Here, 'text-generation' tells the pipeline to create a text generation model, and model='gpt2'\n",
        "# specifies that you want to use the GPT-2 model for this task.\n",
        "generator = pipeline('text-generation', model='gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUv1TbGwJLxI"
      },
      "outputs": [],
      "source": [
        "set_seed(100)\n",
        "input_text = \"I study sociology at University of California\"\n",
        "# specifying the maximum length of the output, and the number of different sequences you want\n",
        "output = generator(input_text, max_length=50, num_return_sequences=5)\n",
        "for result in output:\n",
        "    print(result['generated_text'])\n",
        "    print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxQHEJrLJLxI"
      },
      "source": [
        "# Challenge\n",
        "We can experiment changing different hyper parameters in the model above. Experiment by specifying different values for *temperature* parameter in generator function.\n",
        "<br>\n",
        "Temperature regulates the unpredictability of a language model's output. With higher temperature settings, outputs become more creative and less predictable as it amplifies the likelihood of less probable tokens while reducing that for more probable ones. A temperature value that is higher, such as 1.7, encourages the generation of text that is more varied and imaginative. Conversely, a lower temperature value, like 0.7, steers the output towards more concentrated and predictable text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K69RLiWJJLxI"
      },
      "outputs": [],
      "source": [
        "output = generator(input_text, max_length=50, num_return_sequences=5, .. = ..)\n",
        "for result in output:\n",
        "    print(result['generated_text'])\n",
        "    print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYFbGiuuJLxI"
      },
      "source": [
        "We used GPT2 to generate text above because it is free and runs relatively faster to be able to demonstrate in a classroom setting.  Sometimes, using a larger and more powerful model such as GPT-3 instead of GPT-2 can improve the quality of the generated text. Larger models often capture nuances and subtleties better. However, larger models often take longer time to run and some of the newest models are only available through a paid API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP_oJhO0WcJV"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Let's practice with another task from [huggingface](https://huggingface.co/docs/transformers/task_summary).\n",
        "\n",
        "Let's say we want to check our data for grammatical correctness. We will use the CoLA model (\"textattack/distilbert-base-uncased-CoLA\") in the Text Classification pipeline ('text-classification') What is the grammatical correctness of each of the first 15 entries in the cfpb dataset?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cnh_0zQjfEf"
      },
      "outputs": [],
      "source": [
        "#your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_pNEaqhpKHq"
      },
      "source": [
        "There are thousands of models on huggingface that can be used for a variety of language tasks. This can be a great way to use the models already available to increase our modeling power."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yLbtugXYBqV"
      },
      "source": [
        "# Next Steps\n",
        "\n",
        "This lab has introduced Colab as a way to use GPUs to speed up processing power and explored further applications of deep learning to natural language processing.\n",
        "\n",
        "In practice, using deep learning for computational social science requires building on the foundational concepts covered in this notebook to implement models with more complicated data and architecture. However, there are many strategies can help you navigate the model ecosystem, some of which we will discuss here:\n",
        "\n",
        "1. Documentation (and other resources like tutorials) is a goldmine of information for implementing particular algorithms and completing specific tasks. This is one reason why reading and translating code written by others is a key skill.\n",
        "\n",
        "2. Debugging and interpreting error messages, as well as leveraging online resources in order to resolve them, is another key concept. Resources like documentation and Stack Overflow help solve common errors and get code working faster.\n",
        "\n",
        "3. Computational resources are important for running complex models. Google Colab has access to GPUs, but does have limitations for large and extended jobs. In those cases, options are paid services such as Google Colab Pro or on-campus [resources](https://docs-research-it.berkeley.edu/services/high-performance-computing/overview/).\n",
        "\n",
        "4. Further resources:\n",
        "  - Deep Learning with Python (Francois Chollet)\n",
        "  - [Huggingface course](https://huggingface.co/course/chapter1/1)\n",
        "  - [Tensorflow](https://www.tensorflow.org/tutorials)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "L_zANHRDhplZ",
        "pi2ikVCaoNct",
        "F9xyYBVVRZlE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}